---
title: "Adaptive Model Overview Report"
author: "Pega"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set. That also allows to run the notebook multiple times for different values.
  modelfile:
    # full path to the source file which should be an export of the ADM model table
    # can be a plain CSV file, a zipped CSV or the full path of a dataset export
    #value: "../data/pr_data_dm_admmart_mdl_fact.csv"
    #value: "~/cdh/1/Data-Decision-ADM-ModelSnapshot_pyModelSnapshots_20210317T130409_GMT.zip"
    value: "~/cdh/2/PR_DATA_DM_ADMMART_MDL_FACT.csv"
  predictordatafile:
    # optional full path to ADM predictor binning table data
    # if given, the model overview will also contain aggregate predictor plots
    # this example file is the same as the RDA file used elsewhere in demos of CDH tools
    # can be a plain CSV file, a zipped CSV or the full path of a dataset export
    #value: "../data/pr_data_dm_admmart_pred.csv"
    #value: "~/cdh/1/Data-Decision-ADM-PredictorBinningSnapshot_pyADMPredictorSnapshots_20210317T130452_GMT.zip"
    value: "~/cdh/2/PR_DATA_DM_ADMMART_PRED.csv"
  modellist:
    # optional name for a text file that will be created with a list of model ID and model names
    # to drive bulk-creation of individual model reports
    value: ""
---

```{r, echo=F, warning=F, error=F, include=FALSE}
library(cdhtools)
library(data.table)
library(lubridate)
library(ggplot2)
library(plotly)
library(colorspace)
library(scales)
library(knitr)
library(kableExtra)
library(stringi)

theme_set(theme_minimal())
options(digits = 5)
knitr::opts_chunk$set(
  comment = ">", echo = FALSE, warning = FALSE, fig.width = 10, fig.height = 10
)

source("../r/R/adm.R") # !!!! TEMP !!!!
source("../r/R/plots.R") # !!!! TEMP !!!!
source("../r/R/adm2pmml.R") # !!!! TEMP !!!!
source("../r/R/exportUtilsDatamart.R") # !!!! TEMP !!!!
```


# Overview of Adaptive Models

This notebook gives a global overview of the adaptive models from the data mart. Detailed model reports for individual model instances can be created by running the "modelreport" scripts.

```{r, echo=F, warning=F, error=F, include=F}

if (!("modelfile" %in% names(params))) stop(paste("Required parameter '", modelfile, "' missing."))
if (!file.exists(params$modelfile)) stop(paste("File does not exist:", params$modelfile))

mdls <- readDatamartFromFile(params$modelfile)

```

```{r}
responseCountThreshold <- 2
availableStandardContextKeys <- intersect(c("ConfigurationName", "Issue","Group","Name","Channel","Direction","Treatment"), names(mdls))

mdls[, maxResponseCount := max(Positives), by=availableStandardContextKeys]

droppedMdls <- data.table()
if (nrow(mdls[maxResponseCount < responseCountThreshold]) > 0) {
  droppedMdls <- unique(mdls[maxResponseCount < responseCountThreshold, c(availableStandardContextKeys,"maxResponseCount"), with=F])
  
  mdls <- mdls[maxResponseCount >= responseCountThreshold]
}
```

Dropping `r nrow(droppedMdls)` models with fewer than `r responseCountThreshold` positive responses:

```{r}
droppedMdls
```

```{r}
hasLargeModelList <- (length(unique(mdls$Name)) > 10)
if (hasLargeModelList) {
  propositionLegendForSmallNumbersOnly <- scale_color_discrete_qualitative(guide=F, name="Action")
} else {
  propositionLegendForSmallNumbersOnly <- scale_color_discrete_qualitative(name="Action")
}
```

## Proposition Success Rates

Overall success rate of the propositions. Different channels usually have different success rates.

```{r}
p <- plotADMPropositionSuccessRates(mdls[Positives > 10], limit=20, facets="Channel") + 
  scale_fill_continuous_divergingx()
p
#ggplotly(p) %>% layout(showlegend=FALSE)
```

## Model Performance vs Proposition Success Rates

This is similar to the standard "bubble chart" in the ADM reporting pages.

Note the use of `ggplotly` here: Plotly provides some Javascript based interactivity
and can be applied to any ggplot result. It only works for relatively simple plots and
breaks down especially with faceting, so use in this generic notebook is limited but
it can be great for more specific plots.

```{r message=FALSE}
p <- plotADMPerformanceSuccessRateBubbleChart(mdls, facets = "ConfigurationName")

ggplotly(p) %>% layout(showlegend=FALSE) # %>% config(displayModeBar = F)
```


## Model Performance over Time

```{r}
p <- plotADMModelPerformanceOverTime(mdls) + propositionLegendForSmallNumbersOnly
p
# ggplotly(p)
```

## Model Success Rate over Time

Similar, showing the success rate over time.

The same models that have higher model performance also generally have a higher success rate.

```{r}
p <- plotADMModelSuccessRateOverTime(mdls) + propositionLegendForSmallNumbersOnly

p
#ggplotly(p)
```

# Predictor summaries

Reading the detailed predictor information from the optionally provided file.

This will be used to show predictor info across models.

```{r}
if (is.null(params$predictordatafile) || params$predictordatafile=="")
{
  cat("Predictor related plots will only be available when the predictor data is available to this script.", fill=T)
  modelPredictorBins <- NULL
} else {
  modelPredictorBins <- readDatamartFromFile(params$predictordatafile)
}  
```


## Predictor Performance

### Globally, across all models

Box plots of univariate predictor performance. This gives an indication of the usefulness of 
(categories of) predictors, globally and across all models.

```{r}
if (!is.null(modelPredictorBins) && nrow(modelPredictorBins) > 0) {
  plotADMPredictorPerformance(modelPredictorBins, limit = 40)
} else {
  cat("Predictor related plots will only be available when the predictor data is available to this script.", fill=T)
}
```

### Predictor Importance in models

As measured by the actual contributions to the models they're part of. So unlike
the previous plot that gives a view of the *univariate* performance, this looks
at the actual contribution to the ADM models (using the cumulative log odds of
each of the predictors).

```{r}
varimp <- admVarImp(mdls, modelPredictorBins, facets = "ConfigurationName")
p <- plotADMVarImp(varimp[Rank <= 20])

print(p + scale_fill_continuous_sequential())
```

## Predictor Performance across Propositions

A view of predictor performance across all propositions, ordered so that the best performing predictors are at the top and the 
best performing propositions are on the left. Green indicates good performance, red means more problematic - either too low or
too good to be true.

Instead of faceting, here we loop through the plots since they tend to get large.

```{r, fig.width=8, fig.height=8}
if (!is.null(modelPredictorBins) && nrow(modelPredictorBins) > 0) {
  for (c in unique(mdls$ConfigurationName)) {
    plt <- plotADMPredictorPerformanceMatrix(modelPredictorBins, mdls[ConfigurationName == c], limit=50) +
      theme(axis.text.y = element_text(size=8),
            axis.text.x = element_text(size=8, angle = 45, hjust = 1),
            strip.text = element_text(size=8))
    print(plt)
  }
} else {
  cat("Predictor related plots will only be available when the predictor data is available to this script.", fill=T)
}
```

# Appendix - all the models

```{r}
mdls[, .(Snapshots = .N, Responses = max(ResponseCount)), by=c(availableStandardContextKeys, "ModelID")] #[order(ConfigurationName, Name)]
```

```{r}
# write list of models so the script (createModelReports) to generate off-line model reports can be run after this
if (params$modellist != "") {
  inclKeys <- availableStandardContextKeys[sapply(availableStandardContextKeys, function(x) {return(length(unique(mdls[[x]]))>1)})]
  modelIDandSanitizedNames <- unique(mdls[, .(make.names(apply(.SD, 1, function(x){return(paste(x,collapse="_"))}))), by=ModelID, .SDcols=inclKeys])
  
  write.table(modelIDandSanitizedNames, 
              params$modellist, row.names = F, col.names = F, quote=F, sep=";")
}
```

