---
title: "Adaptive Model Report"
author: "Pega"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set. That also allows to run the notebook multiple times for different values.
  predictordatafile:
    # full path to the source file which should be an export of the ADM predictor binning table
    # this example file is the same as the RDA file used elsewhere in demos of CDH tools
    value: "../data/pr_data_dm_admmart_pred.csv"
    # value: "~/cdh/1/Data-Decision-ADM-PredictorBinningSnapshot_pyADMPredictorSnapshots_20210317T130452_GMT.zip"
    # value: "~/cdh/2/PR_DATA_DM_ADMMART_PRED.csv"
    # value: "~/cdh/3/binning.csv"
    # value: "~/cdh/5/Data-Decision-ADM-PredictorBinningSnapshot_PredictorBinningSnapshotRepo_20201110T084825_GMT.zip"
    # value: "~/cdh/6/predbinning2.csv"
  modeldescription:
    # optional model description typically corresponding to the model info of the indicated model
    value: "Sales Model - PSDISCOUNT100"
  modelid:
    # model ID found by examining the model table from the data mart; if there is just one model
    # in the predictor binning file it can be left empty
    value: "7cf6a839-9eba-5765-8856-1031b1e60315" # default
    #value: "016a8388-9c5f-55cb-85de-7de789a859b2" # 1
    # value: "ccb39edf-eb7d-54c6-8e56-cb3b70d35752"  # 2
    # value: "c70d6b91-1f2f-5680-84d3-f3abbd5112a7" # 3
    # value: "0e4a91ca-137f-5127-a91a-d4249691ccda" # 5
    # value: "aac7fd35-11e8-56db-a85c-fbc1df91d6a8" # 6
    # value: "N/A"
---

```{r, echo=F, warning=F, error=F, include=FALSE}
library(data.table)
library(lubridate)
library(ggplot2)
library(scales)
library(knitr)
library(kableExtra)
library(cdhtools)
library(plotly)
library(readxl)
library(gridExtra)

theme_set(theme_minimal())
options(digits = 5)
knitr::opts_chunk$set(
  comment = ">", echo = FALSE, warning = FALSE, fig.width = 4, fig.height = 4
)
```

# Model report for: `r params$modeldescription` at `r today()`

```{r, echo=F, warning=F, error=F, include=F}

# Code below reads the predictor data and is bloated somewhat to deal with various formatting issues,
# different product versions not always having exactly the same fields etc. In the end, it produces a
# well behaved modelPredictorBins object with the current (latest) snapshot.
# If there are multiple snapshots, predPerformanceOverTime will give that data.

if (!("predictordatafile" %in% names(params))) stop(paste("Required parameter '", predictordatafile, "' missing."))

if (!file.exists(params$predictordatafile)) {
  stop(paste("File does not exist:", params$predictordatafile))
}

## TODO: Excel reading (below) does not generalize very well

# if (endsWith(params$predictordatafile, ".xlsx")) {
#   modelPredictorBins <- as.data.table(read_excel(params$predictordatafile))
#   # The report defs change the column names. Note this might be version dependent.
#   setnames(modelPredictorBins, 
#            old=c("Predictor Name", "Type", "Performance", "Bin Index", "Range/Symbols", "Bin Negatives", "Bin Positives", "Role", "Bin ZRatio", "Bin Lift"), 
#            new=c("PredictorName", "PredictorType", "Performance", "BinIndex", "BinSymbol", "BinNegatives", "BinPositives", "EntryType", "ZRatio", "Lift"))
#   modelPredictorBins[, SnapshotTime := cdhtools::toPRPCDateTime(now())]
#   modelPredictorBins[, ModelID := 0]
#   for (num in c("Lift", "Performance", "BinNegatives", "BinPositives", "BinIndex", "ZRatio")) {
#     modelPredictorBins[[num]] <- as.numeric(modelPredictorBins[[num]])
#   }
# }

modelPredictorBins <- readDatamartFromFile(params$predictordatafile)

# make sure there is only ONE model ID or subset to just the one passed in
if (!(is.null(params$modelid) || (params$modelid %in% c("", "NA", "N/A", "na", "n/a")))) {
  modelPredictorBins <- modelPredictorBins[ModelID == params$modelid]
  if (nrow(modelPredictorBins) <= 1) {
    stop(paste("No data found for model ID", params$modelid))
  }
} else {
  if (length(unique(modelPredictorBins$ModelID)) > 1) {
    stop(paste0("Expected only a single model ID in the data, got ", 
                length(unique(modelPredictorBins$ModelID)), ". Pass in a model ID or split the file."))
  }
}

# # in older versions "PredictorType" was called "Type" - keep both
# if("Type" %in% names(modelPredictorBins) & !"PredictorType" %in% names(modelPredictorBins)) {
#   modelPredictorBins[, PredictorType := Type]
# }
# if(!"Type" %in% names(modelPredictorBins) & "PredictorType" %in% names(modelPredictorBins)) {
#   modelPredictorBins[, Type := PredictorType]
# }

# check for presence of required fields
requiredFields <- c("SnapshotTime","ModelID",
                    "PredictorName","PredictorType","Type","Performance",
                    "BinIndex","BinSymbol","BinNegatives","BinPositives","EntryType","ZRatio","Lift")
requiredFieldsForActualPerformance <- c("BinType", "BinLowerBound","BinUpperBound")
optionalFields <- c("GroupIndex", requiredFieldsForActualPerformance) # not present in all product versions

if (!all(sapply(requiredFields, function(x) { return(x %in% names(modelPredictorBins)) }))) {
  stop(paste("Not all required fields present. Expected:", paste(requiredFields, collapse = ", "), 
             "\ngot:", paste(names(modelPredictorBins), collapse = ", "),
             "\nmissing:", paste(setdiff(requiredFields, names(modelPredictorBins)) , collapse = ", ")))
}

# keep only the required + optional fields in the data so to avoid implicit assumptions
modelPredictorBins <- modelPredictorBins[, intersect(names(modelPredictorBins), c(requiredFields, optionalFields)), with=F]

# # Excel exports sometimes screw up formatting of large numeric values - drop the comma used as thousands separators
# # NB not sure how generic this code will turn out to be
# for (f in c("BinNegatives","BinPositives")) {
#   if (class(modelPredictorBins[[f]]) == "character") {
#     modelPredictorBins[[f]] <- as.numeric(gsub(',','',modelPredictorBins[[f]],fixed=T))
#   }
# }

# Predictor binning can have multiple snapshots. Keeping performance over time but only the last binning.
hasMultipleSnapshots <- (length(unique(modelPredictorBins$SnapshotTime)) > 1)

if (hasMultipleSnapshots) {
  predPerformanceOverTime <- unique(modelPredictorBins[, c("PredictorName", "Performance", "SnapshotTime"), with=F])  
  
  # Take the latest snapshots from the last day. We're doing this carefully as we don't want to report on old bins
  # so just keeping the last day, then per predictor finding the actual last snapshot. This may not work in a situation
  # where not all models are updated frequently.
  
  # # # NB we have seen situations where other formats appeared after import/export to Excel - may need to deal w that a la as.POSIXct(strptime(SnapshotTime, format="%Y-%m-%d"))
  # modelPredictorBins[, snapshot := fromPRPCDateTime(SnapshotTime)] 
  # if (sum(is.na(modelPredictorBins$snapshot))/nrow(modelPredictorBins) > 0.2) {
  #   modelPredictorBins[, snapshot := parse_date_time(SnapshotTime, orders=c("%Y-%m-%d %H:%M:%S"))] 
  #   if (sum(is.na(modelPredictorBins$snapshot))/nrow(modelPredictorBins) > 0.2) {
  #     stop("Assumed Pega date-time string but resulting in over 20% NA's in snapshot time after conversion. Check that this is valid or update the code that deals with date/time conversion.")
  #   }
  # }
  
  lastDay <- max(lubridate::floor_date(modelPredictorBins$SnapshotTime, unit = "days"))
  modelPredictorBins <- modelPredictorBins[lubridate::floor_date(SnapshotTime, unit="days") == lastDay]
  modelPredictorBins <- modelPredictorBins[, .SD[SnapshotTime == max(SnapshotTime)], by=c("ModelID")]
}

# recalculate a few fields that are used - use the naming conventions from the data mart
# NB Performance, Z-Ratio, Lift could have been calculated from the bins but not doing so guarantees consistency with the product reports

modelPredictorBins[, BinResponseCount := (BinPositives+BinNegatives)]
modelPredictorBins[, Positives := sum(BinPositives), by=PredictorName]
modelPredictorBins[, Negatives := sum(BinNegatives), by=PredictorName]

# predictor grouping index was not always there, add it as just a sequence number when absent
if (!("GroupIndex" %in% names(modelPredictorBins))) {
  modelPredictorBins[, GroupIndex := .GRP, by=PredictorName]
}

setorder(modelPredictorBins, -Performance, BinIndex)
```

# Model Performance and Score Distribution

The model scores (sum of the log odds of the Naive Bayes classifier) are mapped to propensities in the Classifier of ADM. This classifier is constructed using the PAV (Pool Adjacent Violators) algorithm, a form of monotonic regression.

## Model Performance

The model reports a performance of `r round(modelPredictorBins[EntryType == "Classifier"]$Performance[1],5)` measured in AUC. If supporting data is available, the actual AUC is recalculated using only those bins that fall into the current score range (if available, shown in parentheses in the title and bins not in range greyed out).

```{r, echo=F, warning=F, error=F}
classifierBinning <- modelPredictorBins[EntryType == "Classifier"]

actualPerformance <- NULL
classifierBinning[, inCurrentRange := T]

# TODO: commented out because it depends heavily on names of fields that have recently changed
# if (all(sapply(requiredFieldsForActualPerformance, function(f) {return(f %in% names(modelPredictorBins))}))) {
#   # TODO consider try/catch because this one easily fails
#   actualPerformance <- getModelPerformanceOverview(dmPredictors = modelPredictorBins)
#   classifierBinning[, inCurrentRange := (BinIndex >= actualPerformance$actual_score_bin_min & BinIndex <= actualPerformance$actual_score_bin_max)]
# }
```

|Total Positives|Total Negatives|Total Responses|Overall Propensity|
|--------------:|--------------:|--------------:|-----------------:|
|`r sum(classifierBinning$BinPositives)`|`r sum(classifierBinning$BinNegatives)`|`r sum(classifierBinning$BinResponseCount)`|`r sprintf("%.2f%%", 100*sum(classifierBinning$BinPositives)/sum(classifierBinning$BinResponseCount))`|

```{r, results="asis", echo=F, warning=F, error=F, fig.align = "center"}
if (nrow(classifierBinning) < 1) {
  cat("<p style='color:Red;'>NO data available for Classifier for date:", max(modelPredictorBins$SnapshotTime), "</p>", fill=T)
}
```

## Cumulative Gains and Lift charts

Below are alternative ways to view the Classifier.

The Cumulative Gains chart shows the percentage of he overall cases in the "positive" category gained by targeting a percentage of the total number of cases. For example, this view shows how large a percentage of the total expected responders you target by targeting only the top decile.

The Lift chart is derived from this and shows the ratio of the cumulative gain and the targeted volume.

```{r, fig.align = "left", fig.width=8, fig.height=3}
# right align is nicer but plotly doesnt do that

# for inspiration on the charts:
# see http://dmg.org/pmml/v4-0-1/ModelExplanation.html#gainscharts
# and https://www.ibm.com/support/knowledgecenter/de/SSLVMB_24.0.0/spss/tutorials/mlp_bankloan_outputtype_02.html

subtitle <- paste0("Performance: ", round(classifierBinning$Performance[1],5), " (AUC)")
if (!is.null(actualPerformance)) {
  if (round(classifierBinning$Performance[1],5) != round(classifierBinning$Performance[1],5)) {
    subtitle <- paste0("Performance: ", round(classifierBinning$Performance[1],5), 
                       " (AUC) (actual: ", round(actualPerformance$actual_performance,5),")")
  }
}

if (nrow(classifierBinning) >= 1 & (sum(classifierBinning$BinNegatives) + sum(classifierBinning$BinPositives) > 0)) {
  cumGains <- plotADMCumulativeGains(classifierBinning)
  
  cumLift <- plotADMCumulativeLift(classifierBinning)
  
  classic <- plotADMBinning(classifierBinning) + ggtitle("Score Distribution")
  
  #ggplotly(cumGains) - TODO convert to plotly plot as ggplotly doesnt show both axes

  grid.arrange(cumGains, cumLift, classic, nrow=1)
}
```

## Score Distribution

The Score Distribution shows the volume and average success rate in every bin of the score ranges of the Classifier. Same plot as before but
this is an interactive one, using Plotly.

```{r fig.align="center", fig.width=8, warning=FALSE, results="asis"}

# Using a native Plotly version of the graph to make the hover-over work. Just 
# plotlygg(p) doesnt work perfectly, unfortunately.

classifierBinning[, bin := factor(BinIndex)]
classifierBinning[, successratepct := 100*BinPositives/BinResponseCount]

if (nrow(classifierBinning) >= 1) {
  ply <- plot_ly(classifierBinning) %>%
    add_bars(x = ~bin, y = ~BinResponseCount, 
             color = ~factor(inCurrentRange, levels=c(T, F)), 
             colors = c("darkgreen", "darkgrey"),
             hoverinfo = "text", 
             text = ~paste0("Score Range: ", BinSymbol, "\nResponses: ", BinResponseCount, "\nSuccess Rate: ", sprintf("%.2f%%", successratepct)),
             yaxis = 'y') %>%
    add_lines(x = ~bin, y = ~successratepct,
              line = list(color = "orange", width = 4), 
              yaxis = 'y2') %>%
    add_markers(x = ~bin[(classifierBinning$inCurrentRange)], y = ~successratepct[(classifierBinning$inCurrentRange)],
                marker = list(color="black"),
                hoverinfo = "text", text = ~sprintf("%.2f%%", successratepct[(classifierBinning$inCurrentRange)]),
                yaxis = 'y2') %>%
    add_markers(x = ~bin[(!classifierBinning$inCurrentRange)], y = ~successratepct[(!classifierBinning$inCurrentRange)],
                marker = list(color="darkgrey"),
                hoverinfo = "text", text = ~sprintf("%.2f%%", successratepct[(!classifierBinning$inCurrentRange)]),
                yaxis = 'y2') %>%
    layout(title = paste0("Score Distribution of the Classifier"),
           xaxis = list(title = ""), # to put values instead of bin indices: , tickangle = -45, tickmode = "array", tickvals = ~bin, ticktext = ~BinSymbol
           yaxis = list(side = 'right', title = "Responses"),
           yaxis2 = list(side = 'left', overlaying = "y", title = 'Success Rate (%)', showgrid = FALSE, zeroline = FALSE, automargin = TRUE, rangemode = "tozero"),
           showlegend = FALSE,
           annotations = list(list(x = 0.5 , y = 1.02, 
                                   text = subtitle, showarrow = F, 
                                   xref='paper', yref='paper'))) %>% 
    config(displayModeBar = F)
  ply
}
```

The success rate is defined as $\frac{positives}{positives+negatives}$ per bin. 

The adjusted propensity that is returned is a small modification (Laplace smoothing) to this and calculated as $\frac{0.5+positives}{1+positives+negatives}$ so empty models return a propensity of 0.5.

```{r, echo=F, warning=F, error=F, include=T}
if (nrow(classifierBinning) >= 1) {
  kable(userFriendlyADMBinning(classifierBinning)) %>% kable_styling()
}
```

# Predictor summary

Number of positives and negatives in each bin and the derived lift and Z-ratio. If grouping information is available, strongly correlated predictors are grouped, with the highest performance predictor groups on top. Groups are indicated by indentation.

```{r, echo=F, warning=F, error=F, include=T}

# TODO - the grouping could be displayed in more fancy ways using kableExtra options for grouping
# TODO - consider colouring the predictor names by part before first dot ; unless there are > 10 of those

predSummary <- modelPredictorBins[EntryType != "Classifier", .(Negatives = sum(BinNegatives),
                                                               Positives = sum(BinPositives),
                                                               Active = EntryType[1],
                                                               Type = PredictorType[1],
                                                               Bins = .N,
                                                               Performance = Performance[1],
                                                               Group = GroupIndex[1]), by=PredictorName]
names(predSummary)[1] <- "Predictor"
if (nrow(predSummary) == 0) {
  cat("The model has no predictors", fill=T)
} else {
  predSummary[, maxGroupPerformance := max(Performance), by=Group]
  setorder(predSummary, -maxGroupPerformance, -Performance)
  predSummary[, isFirstOfGroup := seq(.N)==1, by=Group]
  
  kable(predSummary[,-c("maxGroupPerformance", "isFirstOfGroup")]) %>%
    kable_styling() %>%
    add_indent(which(!predSummary$isFirstOfGroup))
}
```

# Predictor Binning

Binning of all individual predictors. Predictors are listed in the same order as in the summary above.

```{r, results="asis", fig.height = 4, fig.width = 6, fig.align = "center"}

# to print all instead of only the active ones, change condition to: EntryType != "Classifier"

for (f in unique(modelPredictorBins[EntryType == "Active"]$PredictorName)) {
  predictorBinning <- modelPredictorBins[PredictorName==f]

  if (nrow(predictorBinning) < 1) {
    cat("<p style='color:Red;'>NO data available for", f, "for date:", max(modelPredictorBins$SnapshotTime), "</p>", fill=T)
  } else {
    
    # Find other predictors in the same group
    correlatedPreds <- predSummary[Group == predSummary[Predictor==f]$Group & Predictor != f]
    if (nrow(correlatedPreds) > 0) { 
      extraPredInfo <- list("Correlated Predictors" = paste(sort(correlatedPreds$Predictor), collapse = ", "))
    } else {
      extraPredInfo <- list("Correlated Predictors" = "--")
    }
    
    # Table prelude with some overall info about the predictor
    printADMPredictorInfo(predictorBinning, extraPredInfo)

    # colour names: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
    
    if (nrow(predictorBinning) > 1) {
      p <- plotADMBinning(predictorBinning)+ ggtitle(f)
      
      print(p)
    }  
    
    print(kable(userFriendlyADMBinning(predictorBinning), format = "markdown"))
  }
}
```

