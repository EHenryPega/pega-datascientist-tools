---
title: "ADM Standalone Model Report"
title-block-banner: true
author: "Pega data scientist tools"
date: today
subtitle: > 
  Details of one particular ADM model
execute:
  echo: false
format:
  html:
    code-fold: true
    embed-resources: true
    standalone: true
    code-tools: true
    toc: true
    toc-title: Table of Contents
    theme:
      light: flatly
      dark: darkly
jupyter: python3
---

```{python}
from pdstools import datasets, ADMDatamart
from pdstools.utils.cdh_utils import zRatio
import polars as pl
from IPython.display import display, Markdown
from tabulate import tabulate
from itables import show
import os.path
import plotly.express as px
```

```{python}
#| tags: [parameters]

# These parameters are overwritten when called externally

# TODO see if we can align with the params of the Health Check

datafolder = os.path.expanduser("~/Downloads/tmp/")
modelfilename = "Data-Decision-ADM-ModelSnapshot_pyModelSnapshots_20230927T113905_GMT.zip"
predictorfilename = "Data-Decision-ADM-PredictorBinningSnapshot_pyADMPredictorSnapshots_20230927T113944_GMT.zip"
model_id = "99ba769f-1e22-5f26-a7a2-96777b957190"
predictordetails_activeonly = True
```

```{python}
# Predictor data for one model ID
if datafolder == "":
  datamart = datasets.CDHSample()
  model_id = "bd70a915-697a-5d43-ab2c-53b0557c85a0"
else:
  datamart = ADMDatamart(datafolder, model_filename=modelfilename, predictor_filename=predictorfilename, extract_keys=True)

# TODO below seems silly, can probably filter in the constructor arguments
datamart.modelData = datamart.modelData.filter(pl.col("ModelID") == model_id)
datamart.predictorData = datamart.predictorData.filter(pl.col("ModelID") == model_id)

# TODO ensure this is only one predictor snapshot, just in case? although it would be 
# neat to show predictor evolution, if available
```

```{python}
# TODO to make generic make sure these context keys really exist, do intersect with names in the model data

channel_name = datamart.modelData.select(pl.concat_str(["Direction", "Channel"], separator="/")).unique().collect().item()
model_name_in_context = datamart.modelData.select(pl.concat_str(["Issue", "Group", "Name", "Treatment"], separator="/")).unique().collect().item()
model_name = datamart.modelData.select(pl.concat_str(["Name", "Treatment"], separator="/")).unique().collect().item()

display(
    Markdown(
        f"""
# {model_name_in_context}
## {channel_name}
### Internal ID: {model_id}
"""
    )
)
```

```{python}
fig = datamart.plotScoreDistribution(modelids = [model_id])

# Customize some of the default styling of the plot
# TODO except for title consider moving into library
# TODO make the fly-over better
fig.update_layout(title=f"Classifier Score Distribution<br>{model_name}", xaxis_title="")
fig.data[0].opacity=0.5
fig.data[1].line.color="#EF8B08"
fig.data[1].line.width=3
fig.data[1].marker.color="black"

fig.show()
```

::: {.callout-tip}
The charts (built with [Plotly](https://plotly.com/python/)) have [user controls for panning,
zooming etc](https://plotly.com/chart-studio-help/zoom-pan-hover-controls/) but
note that these interactive plots do not render well in portals like Sharepoint
or Box. It is preferable to view them from a browser.
:::

## Model Performance

```{python}
classifier = (
    datamart.predictorData
    .filter(pl.col("EntryType")=="Classifier")
    .sort("BinIndex")
)

auc_roc = round(classifier
    .select(pl.last('Performance'))
    .collect()
    .item(), 4)

display(
    Markdown(
        f"""
The model performance is {auc_roc} measured as AUC-ROC. This number is calculated from the “active” bins of the Classifier.
"""
    )
)

```

The classifier is used to map the model scores (average of the log odds of the active predictors) to a propensity value. The “active” bins are the ones that can be reached from the current binning of the active predictors.

See the [ADM Explained](https://pegasystems.github.io/pega-datascientist-tools/Python/articles/ADMExplained.html) article for more information on how ADM exactly works.

```{python}
#| output: asis

classifier_properties = classifier.select(
    pl.last("ResponseCount").alias("Responses"),
    pl.last("Positives"),
    (pl.last("Positives")*100/pl.last("ResponseCount")).round(3).alias("Base Propensity (%)")
)

show(classifier_properties.collect().to_pandas())
```


## Score Distribution

The Score Distribution shows the volume and average propensity in every bin of
the score ranges of the Classifier.

Propensity is defined as $\frac{positives}{positives+negatives}$ per bin.
The adjusted propensity that is returned is a small modification (*Laplace
smoothing*) to this and calculated as
$\frac{0.5+positives}{1+positives+negatives}$ so new models initially return a
propensity of 0.5. This helps to address the cold start when introducing new
actions.

TODO: unfortunately the Python version has no notion of unreachable bins (yet)

```{python}
#| tbl-cap: Score Distribution

# available fields:
# BinNegatives	BinResponseCount	SnapshotTime	BinPositives	ResponseCount	PredictorName	Type	BinType	ModelID	BinIndex	Contents	Performance	EntryType	Positives	BinSymbol	BinPropensity	BinAdjustedPropensity	PredictorCategory

human_friendly_scoredistribution = (classifier
    .rename({"BinIndex" : "Index", 
             "BinSymbol" : "Bin",
             "BinResponseCount" : "Responses",
             "BinPositives" : "Positives"})
    .select(
        pl.col(["Index", "Bin", "Responses", "Positives"]), 
        (100 * (pl.col("Positives").cumsum(reverse=True)) / pl.sum("Positives")).round(4).alias("Cum. Positives (%)"),
        (100 * (pl.col("Responses").cumsum(reverse=True)) / pl.sum("Responses")).round(4).alias("Cum. Total (%)"),
        (100*pl.col("BinPropensity")).round(3).alias("Propensity (%)"),
        zRatio(pl.col("Positives"), pl.col("Responses") - pl.col("Positives")).round(3).alias("Z Ratio"),
        # TODO perhaps put lift calculation in PDS tools library, it is chopped off from the DM data so we need to recalculate it
        ((pl.col("Positives") * pl.sum("Responses") * 100)/(pl.col("Responses") * pl.sum("Positives"))).round(3).alias("Lift (%)"),
        (100*pl.col("BinAdjustedPropensity")).round(3).alias("Adjusted Propensity (%)"))
).collect()

# Convert to a list of lists, use tabulate to print to ascii art
# the markdown to make it a nice table
# Seems to look better than itables.show() but maybe I'm missing something
# Neither of the two are as good as kable / kableExtra, highlighting of specific
# cells, rows, columns is not easily done

Markdown(
    tabulate([[f[i] for f in human_friendly_scoredistribution] for i in range(human_friendly_scoredistribution.shape[0])],
    headers = human_friendly_scoredistribution.columns))

```


## Cumulative Gains and Lift charts

Below are alternative ways to view the Classifier.

The Cumulative Gains chart shows the percentage of he overall cases in the "positive" category gained by targeting a percentage of the total number of cases. For example, this view shows how large a percentage of the total expected responders you target by targeting only the top decile.

The Lift chart is derived from this and shows the ratio of the cumulative gain and the targeted volume.

TODO: unfortunately the Python version has no notion of unreachable bins (yet) which can cause really strange high lift values

::: {layout-ncol=2}

```{python}
# TODO perhaps this should move into the pdstools plot functions
fig = px.area(
    human_friendly_scoredistribution, 
    x="Cum. Total (%)", y="Cum. Positives (%)", 
    title='Cumulative Gains',
    template="pega")
fig.add_shape(
    type='line', line=dict(dash='dash'),
    x0=0, x1=100, y0=0, y1=100
)
fig.update_yaxes(scaleanchor="x", scaleratio=1, constrain='domain', title=dict(text="% of Positive Responders"))
fig.update_xaxes(constrain='domain', title=dict(text="% of Population"))
fig.show()
```

```{python}
# TODO perhaps this should move into the pdstools plot functions
fig = px.area(
    human_friendly_scoredistribution, 
    x="Cum. Total (%)", y="Lift (%)", 
    title='Lift',
    template="pega")
fig.update_yaxes(scaleanchor="x", scaleratio=0.01, constrain='domain', title=dict(text="Propensity Lift"))
fig.update_xaxes(constrain='domain', title=dict(text="% of Population"))
fig.show()
```

:::

# Trend charts

TODO putting them in tabs is probably only possible if we recreate the graphs as with go, would be nice however

But maybe .data[0] gives the trace


```{python}
fig = datamart.plotOverTime('weighted_performance')

# TODO make the fly-over better
fig.update_layout(title="Model Performance Trend", yaxis_title="ROC-AUC", xaxis_title = "")
fig.update_layout(showlegend=False)

fig.show()
```

```{python}
fig = datamart.plotOverTime('SuccessRate')

# TODO make the fly-over better
fig.update_layout(title="Success Rate Trend", yaxis_title="Success Rate", xaxis_title = "")
fig.update_layout(showlegend=False)
fig.update_yaxes(rangemode="tozero")

fig.show()
```


# Performance by Predictor Category

Showing the performance across all predictors. The predictor categories default
to the text before the first dot. This can be customized when reading the data
for a particular customer.

```{python}
#| error: true

# TODO facets are meaningless but plotPredictorCategoryPerformance fails when
# not giving any

facets = 'Channel'
fig = datamart.plotPredictorCategoryPerformance(facets=facets)

fig.update_layout(
    title="Predictor Performance per Category", 
    yaxis_title="", 
    showlegend=False)

fig.show() 
```

# Predictor Overview

The predictors for this model are sorted by performance and grouped if they are
correlated (shown with an indentation and a lighter color).

The negatives and positives counts are usually the same across all the
predictors but will be different when predictors have been removed or added. IH
predictors often have slightly lower counts.

For Adaptive Gradient Boosting models ("AGB") the number of positives and
negatives is not available.

TODO show table with all the predictors and key properties - should match with the properties we show in the predictor sections, incl missing % etc


```{python}
# TODO this assumes we have the latest snapshot, see above
if predictordetails_activeonly:
    predictors = datamart.predictorData.filter(pl.col("EntryType") == "Active").select(
        pl.col("PredictorName").unique().sort()
    )
else:
    predictors = datamart.predictorData.filter(pl.col("EntryType") != "Classifier").select(
        pl.col("PredictorName").unique().sort()
    )
```

# Binning of the Predictors

The predictors are listed in the same order as in the summary above.

```{python}
display(
    Markdown(
        f"""
Here we show **{'only the active' if predictordetails_activeonly else 'all'}**
predictors. This can be configured via a parameter to this report.
"""
    )
)
```

TODO make the binning plot a little nicer, now the blue bars are too prominent, see the R version. Also reconsider title, etc. Add the alternative view (the red/green bars). Add a table with the binning below. See if we can format the info a bit more compact, like in the R markdown version.

TODO If there are two plots perhaps we can align them with layout-ncol, see https://quarto.org/docs/authoring/figures.html


```{python}
#| output: asis

def show_single_predictor(pred):
    display(Markdown(f"## {pred}"))

    predictor_data = (
        datamart.predictorData
        .filter(pl.col("PredictorName") == pred)
    )

    predictor_properties = (predictor_data
        .select(
            pl.last("Performance").round(4),
            pl.last("EntryType").alias("Status")

            # # TODO figure out how to calculate these easily
            # display(Markdown(f"| Percentage Missing values | TODO |"))
            # display(Markdown(f"| Percentage Residual values | TODO |"))

            # # TODO predictor groups missing https://github.com/pegasystems/pega-datascientist-tools/issues/127
            # display(Markdown(f"| Predictor Group | TODO |"))
            # display(Markdown(f"| Correlated Predictors | TODO |"))
            )
    )

    show(predictor_properties.collect().to_pandas())

    fig = datamart.plotPredictorBinning(modelids=[model_id], predictors=[pred])

    # Customize some of the default styling of the plot
    # TODO except for title consider moving into library
    # TODO make the fly-over better
    fig.update_layout(title=pred, xaxis_title="")
    fig.data[0].opacity=0.5
    fig.data[1].line.color="#EF8B08"
    fig.data[1].line.width=3
    fig.data[1].marker.color="black"

    fig.update_layout(width=700, height=300)
    
    # TODO some plots are totally screwed up, be careful

    fig.show()

    # TODO add the "philip mann" plot here

    # TODO add the simplified binning of the predictors

    human_friendly_binning_table = (
        predictor_data
        .rename({
            "BinIndex" : "Index", 
            "BinSymbol" : "Bin",
            "BinResponseCount" : "Responses",
            "BinPositives" : "Positives"}
        )
        .select(
            pl.col(["Index", "Bin", "Responses", "Positives"]), 
            (pl.col("BinPropensity") * 100).round(3).alias("Propensity (%)"),
            zRatio(pl.col("Positives"), pl.col("Responses") - pl.col("Positives")).round(3).alias("Z Ratio"),
            # TODO See earlier remark on Lift
            ((pl.col("Positives") * pl.sum("Responses") * 100)/(pl.col("Responses") * pl.sum("Positives"))).round(3).alias("Lift (%)")
        )
        .collect()
    )

    display(Markdown(
        tabulate([[f[i] for f in human_friendly_binning_table] for i in range(human_friendly_binning_table.shape[0])],
        headers = human_friendly_binning_table.columns))
    )

    #display(Markdown("---"))

for pred in predictors.collect().to_series(0):
    show_single_predictor(pred)
```


