---
title: "adm-datamart"
author: "Pega"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{adm-datamart}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
```{r setup, include = FALSE}
# knitr options: https://yihui.name/knitr/options/
knitr::opts_chunk$set(
collapse = TRUE,
fig.width = 7,
fig.height = 5,
fig.align = "center",
comment = "#>"
)
```

# ADM Datamart

The ADM Datamart is an open database schema where the ADM models and all predictors are copied into. The standard ADM reports both in Analytics Center as well as in the Report Browser (Adaptive category) work off this schema.

Since the schema is open we can easily use it to create our own reports and support measurements and plots not available in the OOTB reporting in Pega.

# Data Loading

Let's first load the data. An example export is provided with the package. Ways to load your own data from the database or from
a dataset export will be described later.

```{r}
library(cdhtools)

data(admdatamart_models)
data(admdatamart_binning)
```

## Data Loading using Datasets

To load your own data, export the datasets from Pega, download them to your filesystem and let the `cdhtools` library import it. In order to be able to process the data universally we also make sure to make the names lower case and make sure the types are as expected by the snippets that follow below. This would also be a good place to zoom in to specific models relevant to your application.

```{r, eval=FALSE}
admdatamart_models <- readADMDatamartModelExport("~/Downloads")
admdatamart_binning <- readADMDatamartPredictorExport("~/Downloads")

# Filter models relevant to our application
admdatamart_models <- admdatamart_models[ConfigurationName %in% c("SalesModel", "BannerModel")]
```

## Data Loading directly from the database

If you have database access you can also load the data directly from the DB. All you need is a JDBC driver and the proper credentials.

# Advanced Reports

With the data loaded, we can start with a plot similar to what the Analytics Center provides OOTB, however here we can easily include multiple models.

```{r, warning=F, message=FALSE}
library(ggplot2)
library(colorspace)
library(scales)
library(tidyverse)
library(data.table)

ggplot(admdatamart_models %>%
         mutate(Performance = 100*Performance,
                AcceptRate = Positives/ResponseCount),
       aes(Performance, AcceptRate, colour=Name, shape=ConfigurationName, size=log(ResponseCount)))+
  geom_vline(xintercept=c(52,90), linetype="dashed")+
  geom_point(alpha=0.7) +
  guides(colour=guide_legend(title="Proposition"),
         shape=guide_legend(title="Model"),
         size=FALSE)+
  scale_x_continuous(limits = c(50, 70), name = "Proposition Performance") +
  scale_y_continuous(limits = c(0, 1), name = "Success Rate", labels = scales::percent) +
  theme(panel.background = element_rect(fill='lightgreen'))
```

If we group by model instead of proposition, we can see the spread of success rate and performance for all the propositions of one model. In this example, it’s immediately obvious that the Banners have a much lower success rate than the Sales propositions, and also that the Very Simple model has a much lower performance than the (presumably not so simple) Sales model.

```{r, warning=F, message=FALSE}
ggplot(admdatamart_models %>%
         mutate(Performance = 100*Performance,
                Responses = Positives+Negatives,
                AcceptRate = Positives/(Positives+Negatives)),
       aes(Performance, AcceptRate, colour=ConfigurationName))+
  geom_boxplot(alpha=0.7, outlier.shape=8,
               outlier.size=6) +
  guides(colour=guide_legend(title="Model"))+
  scale_x_continuous(limits = c(50, 70), name = "Model Performance") +
  scale_y_continuous(limits = c(0, 1), name = "Success Rate", labels = scales::percent) +
  theme(panel.background = element_rect(fill='lightgreen'))
```

The below plot shows the predictor performance across the selected models. It is similar to one of the reports in the set of out of the box reports available through the Report Browser ("Adaptive" category), however that one does not do a weighted average of the performance. This one does, so gives a more accurate picture.

```{r, warning=F, message=FALSE}
ggplot(admdatamart_binning %>% filter(EntryType != "Classifier") %>%
         group_by(ModelID, PredictorName, Type) %>%
         summarise(Performance = first(Performance),
                   Evidence = sum(BinPositives+BinNegatives)) %>%
         arrange(PredictorName) %>% 
         group_by(PredictorName, Type) %>%
         summarise(WeightedPerformance = sum(Evidence*Performance)/sum(Evidence)) %>%
         arrange(WeightedPerformance),
       aes(factor(PredictorName, levels=unique(PredictorName)), 100*WeightedPerformance, fill=Type)) +
  geom_bar(stat="identity")+
  guides(fill=guide_legend(title="Predictor Type"))+
  coord_flip()+
  theme_bw()+
  xlab("")+
  ggtitle("Weighted avg Predictor Performance")+
  scale_y_continuous(limits=c(50,60),oob = rescale_none, name = "AUC")+
  geom_hline(yintercept=c(52), linetype="dashed")
```

You could also show the predictor performance in one plot across all predictors for all the selected models. Also arranging by performance in both dimensions gives a quick visual clue of which predictors and which models need attention.

```{r, warning=F, message=FALSE}
performanceSummary <- 
  left_join(admdatamart_binning[, .(AUC = first(Performance)), 
                                by=c("ModelID", "SnapshotTime", "PredictorName")],
            admdatamart_models,
            by = c("ModelID","SnapshotTime"))
# maps AUC 0.5 .. 1.0 to a measure of goodness 0 .. 1 just for viz
myGoodness <- function(x)
{
  minOK <- 0.52
  maxOK <- 0.85
  midPt <- 0.3
  return (ifelse(x < minOK, midPt*(x-0.5)/(minOK-0.5), 
                 ifelse(x < maxOK, midPt+(1-midPt)*(x-minOK)/(maxOK-minOK), 
                        1 - (x-maxOK)/(1-maxOK))))
}
## Plot of AUC of predictors x models
meanByModel <- group_by(filter(performanceSummary, PredictorName=="Classifier"), Name) %>% 
  dplyr::summarise(goodness = mean(myGoodness(AUC), na.rm=T),
                   AUC = mean(AUC, na.rm=T)) %>% 
  arrange(desc(goodness))
meanByPredictor <- group_by(performanceSummary, PredictorName) %>% 
  dplyr::summarise(goodness = mean(myGoodness(AUC), na.rm=T),
                   AUC = mean(AUC, na.rm=T)) %>% 
  arrange(goodness)
ggplot(filter(performanceSummary, PredictorName!="Classifier"), 
       aes(x=factor(as.character(Name), levels=meanByModel$Name), 
       y=factor(PredictorName,levels=setdiff(meanByPredictor$PredictorName, "Classifier"))))+
  geom_raster(aes(fill=myGoodness(AUC)))+
  scale_fill_gradient2(low="red", mid="green", high="white", midpoint=0.5) +
  labs(x="",y="",title="Predictors arranged by performance across models")+
  geom_text(aes(label=sprintf("%.2f",100*AUC)), size=2)+
  theme(axis.text.x = element_text(angle = 90, size = 5), axis.text.y = element_text(size = 5))+
  guides(fill=F)
```

# Alternative Performance Metrics

With all the data available in the datamart, you are not restricted to just reporting. Using the binning information for all of the predictors, you could, for example, calculate the KS score of the models. The KS (Kolmogorov–Smirnov) score is related to the AUC that we use by default, but instead of the areas under the ROC curve, it gives the maximum separation between the true positive and false positive rate.

The below snippet calculates the KS score for all predictors of all propositions and shows them next to AUC reported by ADM.


```{r, warning=F, message=FALSE}
admdatamart_binning[ , Propensity := BinPositives/(BinPositives+BinNegatives)]
setorder(admdatamart_binning,Propensity)
myKS <- function(binning)
{
  Goods <- cumsum(binning$BinNegatives) / sum(binning$BinNegatives)
  Bads <- cumsum(binning$BinPositives) / sum(binning$BinPositives)
  KS <- (max(Goods - Bads))
  return(ifelse(is.na(KS),0,KS))
}
performanceSummary <- 
  left_join(admdatamart_binning[, .(KS = myKS(.SD),
                           auc = first(Performance)), 
                       by=c("ModelID", "SnapshotTime", "PredictorName","EntryType")],
            admdatamart_models,
            by = c("ModelID","SnapshotTime"))

## Show KS numbers
ggplot(performanceSummary, 
       aes(Name, factor(PredictorName,
                          levels=c(setdiff(sort(performanceSummary$PredictorName,decreasing = T), "Classifier"),"Classifier"))))+
  geom_raster(aes(fill=(EntryType=="Classifier")))+
  labs(x="",y="",title="Kolmogorov–Smirnov scores")+
  geom_text(aes(label=sprintf("%.2f%%",100*KS)), size=2)+
  scale_fill_brewer(palette="Spectral")+
  guides(fill=F)+
  theme(axis.text.x = element_text(size=6),
        axis.text.y = element_text(size=6))+
  scale_x_discrete(position = "top")
```

# Gains and Lift Charts

The predictor data for the model classifiers can also be used to create gains or lift charts, as an alternative to the bin-oriented propensity plot that is available in the Analytics Center.

For example, Gains charts for the Banner models:

```{r}
classifierdata <- admdatamart_binning[EntryType=="Classifier" & 
                                        ModelID %in% admdatamart_models[ConfigurationName=="BannerModel" &
                                                                            SnapshotTime==max(SnapshotTime)]$ModelID,
                                      c("BinIndex", "BinNegatives", "BinPositives", "Propensity", "Lift", "ModelID")]
setorder(classifierdata,-BinIndex)

classifierdata[,cumRelativeVolume := cumsum((BinNegatives+BinPositives)/sum(BinNegatives+BinPositives)), by=ModelID]
classifierdata[,cumRelativePositives := cumsum((BinPositives)/sum(BinPositives)), by=ModelID]
classifierdata[,Banner := admdatamart_models$Name[admdatamart_models$ModelID == ModelID][1], by=ModelID]

ggplot(classifierdata, aes(cumRelativeVolume, cumRelativePositives, color=Banner)) + geom_line() +
  geom_abline(slope = 1, intercept = 0, color="red", linetype="dashed") +
  scale_x_continuous(limits = c(0, 1), name = "Percentile", labels = scales::percent) +
  scale_y_continuous(limits = c(0, 1), name = "Percentage of Clicks", labels = scales::percent) +
  theme_bw() + guides(color=F) +
  facet_wrap(~Banner)+
  ggtitle("Gains Chart for Banner Models")
```

Or a Lift chart for the same models:

```{r}
classifierdata[,lift := (cumsum(BinPositives)/cumsum(BinNegatives+BinPositives)) / (sum(BinPositives)/sum(BinNegatives+BinPositives)), by=ModelID]

ggplot(classifierdata, aes(cumRelativeVolume, lift, color=Banner)) + geom_line() +
  geom_hline(yintercept = 1, color="red", linetype="dashed") +
  scale_x_continuous(limits = c(0, 1), name = "Percentile", labels = scales::percent) +
  scale_y_continuous(limits = c(0, NA), name = "Lift", labels = scales::percent) +
  theme_bw() + 
  ggtitle("Lift Chart for Banner Models")
```

# Sensitity Analysis

Showing the overall predictor contributions to the models. Behind the scenes
the ADM models are translated to score cards, and the weights of all score card
elements are used to determine the overall weight of the predictors.

Here we "facet" by model rule (ConfigurationName) but you can also leave this
out or facet by e.g. Channel or Issue.

The provided plot function is fairly trivial. For simple use cases it will be
sufficient and the plot can still be decorated using standard ggplot constructs
as shown.

```{r}
varimp <- admVarImp(admdatamart_models, admdatamart_binning, c("ConfigurationName"))
admVarImpPlot(varimp) + 
  theme_bw() + scale_fill_viridis_c(begin=0.5, end=0.9)
```

# Other Ideas

A lot more plots could easily be created. The above are just examples. Showing changes over time would, for example, also be easy to do.

If you have ideas/needs for additional plots please contact us. Create them and put a PR in GitHub or contact us via an Issue.


