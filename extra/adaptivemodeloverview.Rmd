---
title: "Adaptive Model Overview Report"
author: "Pega"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set. That also allows to run the notebook multiple times for different values.
  modelfile:
    # full path to the source file which should be an export of the ADM model table
    # can be a plain CSV file, a zipped up CSV or the full path of a dataset export
    value: "../extra/pr_data_dm_admmart_mdl_fact.csv"
  predictordatafile:
    # optional full path to ADM predictor binning table data
    # if given, the model overview will also contain aggregate predictor plots
    # this example file is the same as the RDA file used elsewhere in demos of CDH tools
    # can be a plain CSV file, a zipped up CSV or the full path of a dataset export
    value: "../extra/pr_data_dm_admmart_pred.csv"
  modellist:
    # optional name for a text file that will be created with a list of model ID and model names
    # to drive bulk-creation of individual model reports
    value: ""
---

```{r, echo=F, warning=F, error=F, include=FALSE}
library(cdhtools)
library(data.table)
library(lubridate)
library(ggplot2)
library(plotly)
library(colorspace)
library(scales)
library(knitr)
library(kableExtra)
library(stringi)

theme_set(theme_minimal())
options(digits = 5)
```


# Overview of Adaptive Models

This notebook gives a global overview of the adaptive models from the data mart. Detailed model reports for individual model instances can be created by running the "modelreport" scripts.

```{r, echo=F, warning=F, error=F, include=F}

knitr::opts_chunk$set(echo=F)

if (!("modelfile" %in% names(params))) stop(paste("Required parameter '", modelfile, "' missing."))

if (!file.exists(params$modelfile)) {
  stop(paste("File does not exist:", params$modelfile))
}

if (endsWith(params$modelfile, ".zip")) {
  # checking if this looks like a DS export
  if (grepl(".*_.*_.*[0-9]{8}T[0-9]{6}_GMT.zip",params$modelfile)) {
    mdls <- readDSExport(params$modelfile)
  } else {
    # NB this might be Linux/Mac only perhaps, consider making configurable
    mdls <- fread(cmd=paste("unzip -p", gsub(" ", "\\ ", params$modelfile, fixed = T))) 
  }
} else {
  mdls <- fread(params$modelfile)
}

# work with lower case names as the various exports processes won't guarantee that case is kept
setnames(mdls, tolower(names(mdls)))
mdls <- mdls[, setdiff(names(mdls), c("pxcommitdatetime", "pzinskey", "pxinsname", "pxobjclass", "pxapplication")), with=F]

# try be smart about the date/time format - is not always Pega format in some of the database exports
suppressWarnings(timez <- fromPRPCDateTime(mdls$pysnapshottime))
if (sum(is.na(timez))/length(timez) > 0.2) {
  suppressWarnings(timez <- parse_date_time(mdls$pysnapshottime, orders=c("%Y-%m-%d %H:%M:%S", "%y-%b-%d") ))    
  if (sum(is.na(timez))/length(timez) > 0.2) {
    stop("Assumed Pega date-time string but resulting in over 20% NA's in snapshot time after conversion. Check that this is valid or update the code that deals with date/time conversion.")
  }
}
mdls[, pysnapshottime := timez]

mdls[, SuccessRate := pypositives/(pypositives+pynegatives)] 
mdls[, Evidence := pypositives+pynegatives] 
mdls[, Performance := 100*as.numeric(pyperformance)] # performance comes out symbolic sometimes

# These are not always there
for (fld in c("pytreatment", "pydirection", "pychannel")) {
  if (!fld %in% names(mdls)) {
    mdls[[fld]] <- ""
  }
}

# with customized model contexts, pyname will be a JSON string, sanitize that a bit
mdls[, pyname := trimws(gsub("\\.+", " ", make.names(paste(".", pyname))))]
```

```{r}
allTimeResponseCountThreshold <- 2
```

Dropping models with fewer than `r allTimeResponseCountThreshold` positive responses:

```{r, warning=F}
stdKeys <- intersect(c("pyconfigurationname", "pyissue","pygroup","pyname","pychannel","pydirection","pytreatment"), names(mdls))

# create an identifier for the full context (just pyname may not be enough to seperate actions/propositions)
mdls[, action := ""]
for (id in setdiff(stdKeys,"pyconfigurationname")) mdls[["action"]] <- paste(mdls[["action"]], mdls[[id]], sep="/")

mdls[, allTimeResponseCount := sum(pypositives), by=stdKeys]

droppedMdls <- "None"
if (nrow(mdls[allTimeResponseCount < allTimeResponseCountThreshold]) > 0) {
  droppedMdls <- unique(mdls[allTimeResponseCount < allTimeResponseCountThreshold, c(stdKeys,"allTimeResponseCount"), with=F])
  
  mdls <- mdls[allTimeResponseCount >= allTimeResponseCountThreshold]
}

droppedMdls
```

```{r}
hasLargeModelList <- (length(unique(mdls$pyname)) > 10)
if (hasLargeModelList) {
  colorScale <- scale_color_discrete_qualitative(guide=F, name="Proposition")
} else {
  colorScale <- scale_color_discrete_qualitative(name="Proposition")
}
latestMdls <- mdls[, 
                   .(Performance = Performance[which.max(pysnapshottime)],
                     SuccessRate = SuccessRate[which.max(pysnapshottime)],
                     Responses = Evidence[which.max(pysnapshottime)]), 
                   by=c("pymodelid", "pyissue","pygroup","pyname","pydirection","pychannel","pytreatment","pyconfigurationname")]

# order by success rate
latestMdls[, pyname := factor(pyname, 
                              levels=latestMdls[, .(SuccessRate = weighted.mean(SuccessRate, Responses, na.rm = T)), by=pyname][order(-SuccessRate)]$pyname)]
```

## Proposition Success Rates

Overall success rate of the propositions. When all models evaluate the same propositions the distributions should not differ much.

```{r}
propSuccess <- latestMdls[, .(SuccessRate = weighted.mean(SuccessRate, Responses, na.rm = T)), 
                          by=c("pyconfigurationname", "pyname")][order(SuccessRate)]
p <- ggplot(propSuccess, aes(factor(pyname, levels=rev(levels(pyname))), 
                             ifelse(is.nan(SuccessRate), 0, SuccessRate), fill=SuccessRate)) + 
  geom_col() + coord_flip() +
  geom_text(aes(label=sprintf("%.2f%%", 100*SuccessRate)), color="blue", hjust=0.5)+
  xlab("Proposition") + scale_y_continuous(name="Success Rate", labels=percent) + 
  ggtitle("Proposition Success Rates") +
  scale_fill_continuous_divergingx(guide=F)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  facet_wrap(~pyconfigurationname, scales = "free_y")
print(p)
```

## Model Performance vs Proposition Success Rates

This is similar to the standard "bubble chart" in the ADM reporting pages.

```{r}
bubbleData <- latestMdls[, .(SuccessRate = weighted.mean(SuccessRate, Responses, na.rm = T),
                             Performance = weighted.mean(Performance, Responses, na.rm = T),
                             Responses = sum(Responses, na.rm = T)), 
                         by=c("pyconfigurationname", "pyname")][order(SuccessRate)]

p <- ggplot(bubbleData, aes(Performance, SuccessRate, size=Responses, colour=pyname)) +
  geom_point()+
  colorScale+
  xlim(c(50,100))+
  facet_wrap(. ~ pyconfigurationname, scales="free_y", strip.position = "right", ncol = 2) +
  scale_y_continuous(labels = scales::percent, name = "Success Rate")+
  ggtitle("Performance vs Success Rate", subtitle = "for latest snapshots")
ggplotly(p) %>% layout(showlegend=FALSE) # %>% config(displayModeBar = F)
```


## Model Performance over Time

```{r}
# order by final performance
mdls[, pyname := factor(pyname, 
                        levels=mdls[, .(Performance = weighted.mean(Performance[which.max(pysnapshottime)], 
                                                                    Evidence[which.max(pysnapshottime)], na.rm = T)), by=pyname][order(-Performance)]$pyname)]

p <- ggplot(mdls[!is.na(pysnapshottime)], aes(pysnapshottime, Performance, color=action)) + geom_line() +
  facet_wrap(. ~ pyconfigurationname, scales="free", strip.position = "right", ncol = 2) +
  ggtitle("Model Performance over Time") +
  colorScale+
  xlab("")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplotly(p)
```

## Model Success Rate over Time

Similar, showing the success rate over time.

The same models that have higher model performance also generally have a higher success rate.

```{r}
# order by final success rate
mdls[, pyname := factor(pyname, 
                        levels=mdls[, .(SuccessRate = weighted.mean(SuccessRate[which.max(pysnapshottime)], 
                                                                    Evidence[which.max(pysnapshottime)], na.rm = T)), by=pyname][order(-SuccessRate)]$pyname)]

p<-ggplot(mdls[!is.na(pysnapshottime)], aes(pysnapshottime, SuccessRate, color=action)) + geom_line() +
  facet_wrap(~ pyconfigurationname, scales = "free") + ggtitle("Proposition Success Rate over Time") +
  colorScale +
  xlab("") + 
  scale_y_continuous(name="Success Rate", labels=percent) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(p)

# p<-ggplot(discretizedView, aes(SnapshotWeek, SuccessRate)) + 
#   geom_line(size=1) +
#   geom_line(aes(y=SuccessRateP10), color="blue", linetype="dashed")+
#   geom_line(aes(y=SuccessRateP90), color="blue", linetype="dashed")+
#   facet_wrap(~ pyconfigurationname) + ggtitle("Proposition Success Rate over Time", subtitle = "Aggregated view with P10 and P90") +
#   colorScale +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# print(p)
```

# Predictor summaries

```{r}
if (params$predictordatafile=="")
{
  cat("Predictor related plots will only be available when the predictor data is available to this script.", fill=T)
  modelPredictors <- NULL
} else {
  if (!file.exists(params$predictordatafile)) {
    stop(paste("File does not exist:", params$predictordatafile))
  }
  if (endsWith(params$predictordatafile, ".zip")) {
    # checking if this looks like a DS export
    if (grepl(".*_.*_.*[0-9]{8}T[0-9]{6}_GMT.zip",params$predictordatafile)) {
      modelPredictorBins <- readDSExport(params$predictordatafile)
    } else {
      # NB this might work only on Linux/Mac; consider making configurable
      modelPredictorBins <- fread(cmd=paste("unzip -p", gsub(" ", "\\ ", params$predictordatafile, fixed = T))) 
    }
  } else {
    modelPredictorBins <- fread(params$predictordatafile)
  }
  
  # work with lower case names as the various exports processes won't guarantee that case is kept
  setnames(modelPredictorBins, tolower(names(modelPredictorBins)))
  
  # in older versions "pypredictortype" was called "pytype"
  if("pytype" %in% names(modelPredictorBins) & !"pypredictortype" %in% names(modelPredictorBins)) {
    modelPredictorBins[, pypredictortype := pytype]
  }
  
  # Excel exports sometimes screw up formatting of large numeric values - drop the comma used as thousands separators
  # NB not sure how generic this code will turn out to be
  for (f in c("pybinnegatives","pybinpositives")) {
    if (class(modelPredictorBins[[f]]) == "character") {
      modelPredictorBins[[f]] <- as.numeric(gsub(',','',modelPredictorBins[[f]],fixed=T))
    }
  }
  
  # only for the current models, only a few fields are of interest and aggregate bin counts to predictor level
  fieldsOfInterest <- c("pymodelid", "pysnapshottime", "pypredictorname", "pypredictortype", "pybinnegatives","pybinpositives","pybinindex")
  modelPredictors <- modelPredictorBins[pymodelid %in% mdls$pymodelid & pyentrytype != "Classifier", 
                                        .(pypredictortype = first(pypredictortype),
                                          pyentrytype = first(pyentrytype),
                                          pyperformance = 100*auc_from_bincounts(pybinpositives, pybinnegatives),
                                          pynegatives = sum(pybinnegatives),
                                          pypositives = sum(pybinpositives),
                                          pyresponsecount = sum(pybinnegatives)+sum(pybinpositives)), 
                                        by=c("pymodelid", "pysnapshottime", "pypredictorname")]
  
  # allClassifierBins <- merge(modelPredictorBins[pyentrytype == "Classifier", 
  #                                         c("pymodelid", "pybinindex", "pybinpositives", "pybinnegatives", "pybinlowerbound", "pybinupperbound", "pybinsymbol"), with=F],
  #                            unique(mdls[, c("pymodelid", "pyissue", "pygroup")]),
  #                            by="pymodelid")
                             
  rm(modelPredictorBins)
  
  # only keep latest snapshot if there are multiple
  if (length(unique(modelPredictors$pysnapshottime)) > 1) {
    
    #modelPredictors[, snapshot := fromPRPCDateTime(pysnapshottime)] 
    
    # which.max does not work for symbolics, so doing it the dumb way
    modelPredictors <- modelPredictors[, .SD[which(pysnapshottime==max(pysnapshottime))], by=c("pymodelid", "pypredictorname")] # TODO not sure if this is correct
  }
  
  # abbreviate lengthy names
  modelPredictors[, pypredictorname := factor(pypredictorname)]
  
  abbrev <- function(str)
  {
    len <- 32
    parts <- strsplit(str,".",fixed=T)  
    if (length(parts[[1]]) == 1) return(str)
    rhs <- paste(parts[[1]][2:length(parts[[1]])],collapse=".")
    if (nchar(rhs) < len) return(str)
    return(paste(parts[[1]][1], stri_sub(rhs,-len,-1), sep="..."))
  }
  
  modelPredictors[, predictorname_ori := pypredictorname]
  levels(modelPredictors$pypredictorname) <- sapply(levels(modelPredictors$pypredictorname), abbrev)
  
  if (length(unique(levels(modelPredictors$pypredictorname))) != 
      length(unique(levels(modelPredictors$predictorname_ori)))) 
  {
    # abbreviation would loose predictors, revert
    modelPredictors[, pypredictorname := predictorname_ori]
  }
  
  modelPredictors[, pypredictorname := as.character(pypredictorname)]

  # finally join with model data
  modelPredictors <- merge(modelPredictors, unique(mdls[, c("pymodelid", "pyconfigurationname", "pyname"), with=F]), by="pymodelid", all.x=T, all.y=F)
}
```


## Predictor performance across models

```{r, fig.width=10, fig.height=10}
if (!is.null(modelPredictors) && nrow(modelPredictors) > 0) {
  predOrder <- modelPredictors[, .(meanPerf = weighted.mean(pyperformance, 1+pyresponsecount, na.rm = T)), 
                               by=pypredictorname][order(meanPerf)] $ pypredictorname
  
  modelPredictors[, predictorCategory := sapply(strsplit(pypredictorname, ".", fixed=T),
                                                function(x) {return(ifelse(length(x)<2,"Primary",x[[1]]))})]
  modelPredictors[, predictorType := factor(pypredictortype, levels = c("numeric", "symbolic"))]
  
  p <- ggplot(modelPredictors, aes(factor(pypredictorname, levels=predOrder), pyperformance)) + 
    geom_boxplot(lwd=1) +
    coord_flip() +
    facet_wrap(. ~ pyconfigurationname, scales="free_y", strip.position = "right", ncol = 2) +
    ylab("Predictor Performance") + xlab("") + 
    ggtitle("Predictor performance distribution", subtitle = "Across all models") +
    ylim(c(50,NA))+
    theme(axis.text.y = element_text(size=8),
          strip.text = element_text(size=8))  
  if (length(unique(modelPredictors$predictorCategory))>1) {
    # use fill as well as line color
    p <- p + geom_boxplot(mapping = aes(fill=predictorCategory, color=predictorType), lwd=0.5) +
      scale_fill_discrete_divergingx(name="Category") +
      scale_color_discrete_diverging(name="Predictor Type")
  } else {
    # only type available
    p <- p + geom_boxplot(mapping = aes(fill=predictorType), lwd=0.5) +
      scale_fill_discrete_divergingx(name="Predictor Type")
  }
  ggplotly(p)
} else {
  cat("Predictor related plots will only be available when the predictor data is available to this script.", fill=T)
}
```

## Predictor Performance across Propositions

A view of predictor performance across all propositions, ordered so that the best performing predictors are at the top and the 
best performing propositions are on the left. Green indicates good performance, red means more problematic - either too low or
too good to be true.

```{r, fig.width=10, fig.height=10}
if (!is.null(modelPredictors) && nrow(modelPredictors) > 0) {
  myGoodness <- function(x)  
  {  
    minOK <- 52  
    maxOK <- 85  
    midPt <- 30  
    return (ifelse(x < minOK, midPt*(x-50)/(minOK-50),  
                   ifelse(x < maxOK, midPt+(100-midPt)*(x-minOK)/(maxOK-minOK),  
                          100 - (x-maxOK)/(100-maxOK))))  
  }  
  
  perfPredVsProp <- modelPredictors[, .(Performance = weighted.mean(pyperformance, 1+pyresponsecount, na.rm = T)), 
                                    by=c("pyconfigurationname", "pypredictorname", "pyname")]
  
  propOrder <- modelPredictors[, .(meanPerf = weighted.mean(pyperformance, 1+pyresponsecount, na.rm = T)), 
                               by=pyname][order(-meanPerf)] $ pyname
  
  ggplot(perfPredVsProp, aes(factor(pyname, levels=propOrder), factor(pypredictorname, levels=predOrder))) +
    geom_raster(aes(fill=myGoodness(Performance))) +
    facet_wrap(. ~ pyconfigurationname, scales="free", strip.position = "right", ncol = 2) +
    scale_fill_gradient2(low="red", mid="green", high="white", midpoint=50) +  
    labs(x="",y="",title="Predictors Performance", subtitle = "by Proposition")+  
    geom_text(aes(label=sprintf("%.2f",Performance)), size=3)+  
    theme(axis.text.y = element_text(size=8),
          axis.text.x = element_text(size=8, angle = 45, hjust = 1),
          strip.text = element_text(size=8)) +
    guides(fill=F)
} else {
  cat("Predictor related plots will only be available when the predictor data is available to this script.", fill=T)
}
```

# Appendix - all the models

```{r}
kable(unique(mdls[, c("pymodelid","pyconfigurationname","pyname","action"), with=F])[order(pyconfigurationname, pyname)][,-"pyname"])
```

```{r}
# write list of models so the script (createModelReports) to generate off-line model reports can be run after this
if (params$modellist != "") {
  inclKeys <- stdKeys[sapply(stdKeys, function(x) {return(length(unique(mdls[[x]]))>1)})]
  modelIDandSanitizedNames <- unique(mdls[, .(make.names(apply(.SD, 1, function(x){return(paste(x,collapse="_"))}))), by=pymodelid, .SDcols=inclKeys])
  
  write.table(modelIDandSanitizedNames, 
              params$modellist, row.names = F, col.names = F, quote=F, sep=";")
}
```

