---
title: "Adaptive Model Report"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set. That also allows to run the notebook multiple times for different values.
  predictordatafile:
    # full path to the source file which should be an export of the ADM predictor binning table
    # this example file is the same as the RDA file used elsewhere in demos of CDH tools
    value: "../extra/pr_data_dm_admmart_pred.csv"
  modeldescription:
    # optional model description typically corresponding to the model info of the indicated model
    value: "Sales Model - PSDISCOUNT100"
  modelid:
    # model ID found by examining the model table from the data mart; if there is just one model
    # in the predictor binning file it can be left empty
    value: "7cf6a839-9eba-5765-8856-1031b1e60315"
---

```{r, echo=F, warning=F, error=F, include=FALSE}
library(data.table)
library(lubridate)
library(ggplot2)
library(scales)
library(knitr)
library(kableExtra)
library(cdhtools)
library(plotly)
```

# Model report for: `r params$modeldescription` at `r today()`

```{r, echo=F, warning=F, error=F, include=F}

# Code below reads the predictor data and is bloated somewhat to deal with various formatting issues,
# different product versions not always having exactly the same fields etc. In the end, it produces a
# well behaved modelPredictorBins object with the current (latest) snapshot.
# If there are multiple snapshots, predPerformanceOverTime will give that data.

if (!file.exists(params$predictordatafile)) {
  stop(paste("File does not exist:", params$predictordatafile))
}
if (endsWith(params$predictordatafile, ".zip")) {
  # NB this might work only on Linux/Mac; consider making configurable
  modelPredictorBins <- fread(cmd=paste("unzip -p", gsub(" ", "\\ ", params$predictordatafile, fixed = T))) 
} else {
  modelPredictorBins <- fread(params$predictordatafile)
}

# work with lower case names as the various exports processes won't guarantee that case is kept
setnames(modelPredictorBins, tolower(names(modelPredictorBins)))

# make sure there is only ONE model ID or subset to just the one passed in
if (params$modelid != "") {
  modelPredictorBins <- modelPredictorBins[pymodelid == params$modelid]
  if (nrow(modelPredictorBins) <= 1) {
    stop(paste("No data found for model ID", params$modelid))
  }
} else {
  if (length(unique(modelPredictorBins$pymodelid)) > 1) {
    stop(paste0("Expected only a single model ID in the data, got ", 
                length(unique(modelPredictorBins$pymodelid)), ". Pass in a model ID or split the file."))
  }
}

# in older versions "pypredictortype" was called "pytype" - keep both
if("pytype" %in% names(modelPredictorBins) & !"pypredictortype" %in% names(modelPredictorBins)) {
  modelPredictorBins[, pypredictortype := pytype]
}
if(!"pytype" %in% names(modelPredictorBins) & "pypredictortype" %in% names(modelPredictorBins)) {
  modelPredictorBins[, pytype := pypredictortype]
}

# check for presence of required fields
requiredFields <- c("pysnapshottime","pymodelid",
                    "pypredictorname","pypredictortype","pytype","pyperformance",
                    "pybinindex","pybinsymbol","pybinnegatives","pybinpositives","pyentrytype","pyzratio","pylift")
requiredFieldsForActualPerformance <- c("pybintype", "pybinlowerbound","pybinupperbound")
optionalFields <- c("pygroupindex", requiredFieldsForActualPerformance) # not present in all product versions

if (!all(sapply(requiredFields, function(x) { return(x %in% names(modelPredictorBins)) }))) {
  stop(paste("Not all required fields present. Expected:", paste(requiredFields, collapse = ", "), 
             "\ngot:", paste(names(modelPredictorBins), collapse = ", "),
             "\nmissing:", paste(setdiff(requiredFields, names(modelPredictorBins)) , collapse = ", ")))
}

# keep only the required + optional fields in the data so to avoid implicit assumptions
modelPredictorBins <- modelPredictorBins[, intersect(names(modelPredictorBins), c(requiredFields, optionalFields)), with=F]

# Excel exports sometimes screw up formatting of large numeric values - drop the comma used as thousands separators
# NB not sure how generic this code will turn out to be
for (f in c("pybinnegatives","pybinpositives")) {
  if (class(modelPredictorBins[[f]]) == "character") {
    modelPredictorBins[[f]] <- as.numeric(gsub(',','',modelPredictorBins[[f]],fixed=T))
  }
}

# Predictor binning can have multiple snapshots. Keeping performance over time but only the last binning.
hasMultipleSnapshots <- (length(unique(modelPredictorBins$pysnapshottime)) > 1)
if (hasMultipleSnapshots) {
  predPerformanceOverTime <- unique(modelPredictorBins[, c("pypredictorname", "pyperformance", "pysnapshottime"), with=F])  
  
  # Take the latest snapshots from the last day. We're doing this carefully as we don't want to report on old bins
  # so just keeping the last day, then per predictor finding the actual last snapshot. This may not work in a situation
  # where not all models are updated frequently.
  
  # # NB we have seen situations where other formats appeared after import/export to Excel - may need to deal w that a la as.POSIXct(strptime(pysnapshottime, format="%Y-%m-%d"))
  modelPredictorBins[, snapshot := fromPRPCDateTime(pysnapshottime)] 
  if (sum(is.na(modelPredictorBins$snapshot))/nrow(modelPredictorBins) > 0.2) {
    stop("Assumed Pega date-time string but resulting in over 20% NA's in snapshot time after conversion. Check that this is valid or update the code that deals with date/time conversion.")
  }
  lastDay <- max(lubridate::floor_date(modelPredictorBins$snapshot, unit = "days"))
  modelPredictorBins <- modelPredictorBins[lubridate::floor_date(snapshot, unit="days") == lastDay]
  modelPredictorBins <- modelPredictorBins[, .SD[snapshot == max(snapshot)], by=c("pymodelid")]
}

# recalculate a few fields that are used - use the naming conventions from the data mart
# NB Performance, Z-Ratio, Lift could have been calculated from the bins but not doing so guarantees consistency with the product reports

modelPredictorBins[, pybinresponsecount := (pybinpositives+pybinnegatives)]
modelPredictorBins[, pypositives := sum(pybinpositives), by=pypredictorname]
modelPredictorBins[, pynegatives := sum(pybinnegatives), by=pypredictorname]

# predictor grouping index was not always there, add it as just a sequence number when absent
if (!("pygroupindex" %in% names(modelPredictorBins))) {
  modelPredictorBins[, pygroupindex := .GRP, by=pypredictorname]
}

setorder(modelPredictorBins, -pyperformance, pybinindex)
```

# Model Performance and Propensity Mapping

The model scores (sum of the log odds of the Naive Bayes classifier) are mapped to propensities in the Classifier of ADM. This classifier is constructed using the PAV (Pool Adjacent Violaters) algorithm, a form of monotonic regression.

The model reports a performance of `r round(modelPredictorBins[pyentrytype == "Classifier"]$pyperformance[1],5)` measured in AUC. If supporting data is available, the actual AUC is recalculated using only those bins that fall into the current score range (if available, shown in parentheses in the title and bins not in range greyed out).

```{r, echo=F, warning=F, error=F}
binning <- modelPredictorBins[pyentrytype == "Classifier"]

actualPerformance <- NULL
binning[, inCurrentRange := NA]
if (all(sapply(requiredFieldsForActualPerformance, function(f) {return(f %in% names(modelPredictorBins))}))) {
  # TODO consider try/catch because this one easily fails
  actualPerformance <- getModelPerformanceOverview(dmPredictors = modelPredictorBins)
  binning[, inCurrentRange := (pybinindex >= actualPerformance$actual_score_bin_min & pybinindex <= actualPerformance$actual_score_bin_max)]
}

binning[, bin := factor(pybinindex)]
binning[, successrate := pybinpositives/pybinresponsecount]
binning[, successratepct := 100*successrate]
binning[, pylift := 100*pylift]
binning[, adjustedpropensity := 100*(0.5+pybinpositives)/(1+pybinresponsecount)]
```

|Total Positives|Total Negatives|Total Responses|Overall Propensity|
|--------------:|--------------:|--------------:|-----------------:|
|`r sum(binning$pybinpositives)`|`r sum(binning$pybinnegatives)`|`r sum(binning$pybinpositives) + sum(binning$pybinnegatives)`|`r sprintf("%.2f%%", 100*sum(binning$pybinpositives)/(sum(binning$pybinpositives) + sum(binning$pybinnegatives)))`|

```{r, results="asis", echo=F, warning=F, error=F, fig.align = "center"}
if (nrow(binning) < 1) {
  cat("<p style='color:Red;'>NO data available for Classifier for date:", max(modelPredictorBins$pysnapshottime), "</p>", fill=T)
} else {
  
  # Not using the ggplot version - replaced by the plotly version below!
  
  successRateMax <- max(binning$successrate, na.rm = T)
  if (0 == successRateMax) { successRateMax <- 1 }
  secAxisFactor <- max(binning$pybinresponsecount)/successRateMax
  
  if (0 == secAxisFactor) {
    p <- ggplot(binning, aes(bin, pybinresponsecount)) +
      geom_col(fill = "darkgreen") +
      scale_y_continuous(limits=c(0, max(binning$successrate)), name="Success Rate", labels=percent,
                         sec.axis = sec_axis(~.*secAxisFactor, name = "Responses")) +
      ggtitle("Score Distribution", subtitle = paste0(binning$pyentrytype[1], "; Performance=", binning$pyperformance[1]))
    
  } else {
    p <- ggplot(binning, aes(bin, successrate, group=1))
    
    if (is.null(actualPerformance)) {
      p <- p + 
        geom_col(aes(y=pybinresponsecount/secAxisFactor), fill = "darkgreen") +
        geom_line(colour="orange", size=2)+
        geom_point(color = "black") +
        ggtitle("Score Distribution of the Classifier", 
                subtitle = paste0("Performance=", round(binning$pyperformance[1],5)))
    } else {
      p <- p + 
        geom_col(aes(y=pybinresponsecount/secAxisFactor, fill = factor(inCurrentRange, levels=c(T, F))))+
        geom_line(colour="orange", size=2)+
        scale_fill_manual(values=c("darkgreen", "darkgrey"), guide=F)+
        geom_point(aes(color = factor(inCurrentRange, levels=c(T, F))))+
        scale_color_manual(values=c("black", "darkgrey"), guide=F)+
        geom_vline(xintercept = c(actualPerformance$actual_score_bin_min-0.5, xmax=actualPerformance$actual_score_bin_max+0.5), color="blue")+
        ggtitle("Score Distribution of the Classifier", 
                subtitle = paste0("Performance=", round(binning$pyperformance[1],5), " (", round(actualPerformance$actual_performance,5),")"))
    }
    p <- p + 
      scale_y_continuous(limits=c(0, max(binning$successrate)), name="Success Rate", labels=percent,
                         sec.axis = sec_axis(~.*secAxisFactor, name = "Responses")) +
      scale_x_discrete(name = "", labels=binning$pybinsymbol) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
  }
  
  # Not using the ggplot version - replaced by the plotly version below!
  # print(p)
}
```

```{r, results="asis", echo=F, warning=F, error=F, fig.align = "center"}
# Plotly version of the graph gives hover-over. Just plotlygg(p) doesnt work perfectly, unfortunately, so building it up.

if (is.null(actualPerformance)) {
  subtitle <- paste0("Performance: ", round(binning$pyperformance[1],5), " (AUC)")
} else {
  subtitle <- paste0("Performance: ", round(binning$pyperformance[1],5), " (AUC) (actual: ", round(actualPerformance$actual_performance,5),")")
}

if (nrow(binning) >= 1) {
  binning[, Responses := pybinpositives+pybinnegatives]
  ply <- plot_ly(binning) %>%
    add_bars(x = ~bin, y = ~Responses, 
             color = ~factor(inCurrentRange, levels=c(T, F)), 
             colors = c("darkgreen", "darkgrey"),
             hoverinfo = "text", 
             text = ~paste0("Score Range: ", pybinsymbol, "\nResponses: ", Responses, "\nSuccess Rate: ", sprintf("%.2f%%", successratepct))) %>%
    add_trace(x = ~bin, y = ~successratepct,
              type = 'scatter', mode = 'lines+markers',
              hoverinfo = "text", text = ~sprintf("%.2f%%", successratepct),
              line = list(color = "orange", width = 4), 
              marker = list(color = "black"),
              yaxis = 'y2') %>%
    layout(title = paste0("Score Distribution of the Classifier"),
           xaxis = list(title = ""), # to put values instead of bin indicdes: , tickangle = -45, tickmode = "array", tickvals = ~bin, ticktext = ~pybinsymbol
           yaxis = list(side = 'left', title = "Responses"),
           yaxis2 = list(side = 'right', overlaying = "y", title = 'Success Rate (%)', showgrid = FALSE, zeroline = FALSE, automargin = TRUE, rangemode = "tozero"),
           showlegend = FALSE,
           annotations = list(list(x = 0.5 , y = 1.02, text = subtitle, showarrow = F, xref='paper', yref='paper'))) %>% 
    config(displayModeBar = F)
  ply
}
```

The success rate is defined as $\frac{positives}{positives+negatives}$ per bin. 

The adjusted propensity that is returned is a small modification (Laplace smoothing) to this and calculated as $\frac{0.5+positives}{1+positives+negatives}$ so empty models return a propensity of 0.5.

```{r, echo=F, warning=F, error=F, include=T}
if (nrow(binning) >= 1) {
  binningTable <- binning[, c("pybinindex", "pybinsymbol", "pybinpositives", "pybinnegatives", "successratepct", "adjustedpropensity", "pyzratio", "pylift", "inCurrentRange"), with=F]
  setnames(binningTable, c("Index", "Bin", "Positives", "Negatives", "Success Rate (%)", "Adjusted Propensity (%)", "Z-Ratio", "Lift (%)", "In Current Range"))
  
  kable(binningTable) %>% kable_styling()
}
```

# Predictor summary

Number of positives and negatives in each bin and the derived lift and Z-ratio. If grouping information is available, strongly correlated predictors are grouped, with the highest performance predictor groups on top. Groups are indicated by indentation.

```{r, echo=F, warning=F, error=F, include=T}

# TODO - the grouping could be displayed in more fancy ways using kableExtra options for grouping
# TODO - consider colouring the predictor names by part before first dot ; unless there are > 10 of those

predSummary <- modelPredictorBins[pyentrytype != "Classifier", .(Negatives = sum(pybinnegatives),
                                                                 Positives = sum(pybinpositives),
                                                                 Active = pyentrytype[1],
                                                                 Type = pypredictortype[1],
                                                                 Bins = .N,
                                                                 Performance = pyperformance[1],
                                                                 Group = pygroupindex[1]), by=pypredictorname]
names(predSummary)[1] <- "Predictor"
predSummary[, maxGroupPerformance := max(Performance), by=Group]
setorder(predSummary, -maxGroupPerformance, -Performance)
predSummary[, isFirstOfGroup := seq(.N)==1, by=Group]

kable(predSummary[,-c("maxGroupPerformance", "isFirstOfGroup")]) %>%
  kable_styling() %>%
  add_indent(which(!predSummary$isFirstOfGroup))
```

# Predictor Binning

Binning of all individual predictors. Predictors are listed in the same order as in the summary above.

```{r, results="asis", echo=F, warning=F, error=F, fig.height = 5, fig.width = 6, fig.align = "center"}
for (f in unique(modelPredictorBins[pyentrytype != "Classifier"]$pypredictorname)) {
  binning <- modelPredictorBins[pypredictorname==f]
  
  if (nrow(binning) < 1) {
    cat("<p style='color:Red;'>NO data available for", f, "for date:", max(modelPredictorBins$pysnapshottime), "</p>", fill=T)
  } else {
    
    # Table prelude with some overall info about the predictor
    cat(paste0("\n<p></p>## ", f, "\n<p></p>"))
    cat("\n<p></p>|Field|Value|\n")
    cat("|---|---|\n")
    cat(paste0("|Univariate Performance (AUC)|",binning$pyperformance[1],"|\n"))
    cat(paste0("|Status|",binning$pyentrytype[1],"|\n"))
    cat(paste0("|Predictor Group|",binning$pygroupindex[1],"|\n"))
    cat(paste0("|Total Positives|",sum(binning$pybinpositives),"|\n"))
    cat(paste0("|Total Negatives|",sum(binning$pybinnegatives),"|\n"))
    cat(paste0("|Total Responses|",sum(binning$pybinpositives)+sum(binning$pybinnegatives),"|\n"))
    cat(paste0("|Overall Propensity|",sprintf("%.2f%%",100*sum(binning$pybinpositives)/(sum(binning$pybinpositives) + sum(binning$pybinnegatives))),"|\n"))
    cat("<p></p>")
    
    # A list with the other predictors in the same group
    correlatedPreds <- predSummary[Group == predSummary[Predictor==f]$Group & Predictor != f]
    if (nrow(correlatedPreds) > 0) { 
      cat(paste0("Correlated Predictors: ", paste(correlatedPreds$pypredictorname, collapse = ", "), "\n<p></p>"))
    }
    
    binning[, bin := factor(pybinindex)]
    binning[, successrate := pybinpositives/pybinresponsecount]
    binning[, successratepct := 100*successrate]
    binning[, pylift := 100*pylift]
    
    successRateMax <- max(binning$successrate, na.rm = T)
    if (0 == successRateMax) { successRateMax <- 1 }
    secAxisFactor <- max(binning$pybinresponsecount)/successRateMax
    # colour names: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
    
    if (nrow(binning) > 1) {
      p <- ggplot(binning, aes(bin, successrate, group=1))+
        geom_col(aes(y=pybinresponsecount/secAxisFactor), fill=ifelse(binning$pyentrytype[1]=="Active","steelblue3","darkgrey"))+
        geom_line(colour="orange", size=2)+geom_point()+
        geom_hline(data=binning[1,], mapping = aes(yintercept = pypositives/(pypositives+pynegatives)),
                   colour="orange", linetype="dashed") +
        scale_y_continuous(limits=c(0, successRateMax), name="Success Rate", labels=percent,
                           sec.axis = sec_axis(~.*secAxisFactor, name = "Responses"))+
        scale_x_discrete(name = "", 
                         labels=ifelse(binning$pypredictortype == "numeric" | nchar(binning$pybinsymbol) <= 25, 
                                       binning$pybinsymbol, 
                                       paste(substr(binning$pybinsymbol, 1, 25), "..."))) +
        ggtitle(f)+
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
      print(p)
    }  
    
    binningTable <- binning[, c("pybinindex", "pybinsymbol", "pybinpositives", "pybinnegatives", "successratepct", "pyzratio", "pylift"), with=F]
    setnames(binningTable, c("Index", "Bin", "Positives", "Negatives", "Success Rate (%)", "Z-Ratio", "Lift (%)"))
    print(kable(binningTable, format = "markdown"))
  }
}
```

