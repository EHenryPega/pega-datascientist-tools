---
title: "`r params$title`"
subtitle: "`r params$modeldescription`"
author: "Pega"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
always_allow_html: true
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set. That also allows to run the notebook multiple times for different values.
  
  title: "Adaptive Model Report"
  predictordatafile:
   # Full path to ADM predictor binning table data. Can be CSV, zipped CSV,
   # LD-JSON, parquet or more.
   value: "../../data/pr_data_dm_admmart_pred.csv"

  modeldescription:
    # Optional model description typically corresponding to the model info of the indicated model
    value: "Sales Model - PSDISCOUNT100"
    
  modelid:
    # Model ID that is used to filter the predictor binning table data. If the
    # binning data only contains a single Model ID it can be left empty.
    value: "68e1d164-81e3-5da0-816c-0bbc3c20ac6c" # default

  # Showing the binning of the predictors for all or only for the active ones.
  predictordetails_activeonly: true
---

```{r, echo=F, warning=F, error=F, include=FALSE}
library(pdstools)

# include below when developing the library
# sapply(list.files("~/Documents/pega/pega-datascientist-tools/r/R", "*.R", full.names = T), source)

library(data.table)
library(lubridate)
library(ggplot2)
library(scales)
library(knitr)
library(kableExtra)
library(plotly)
library(gridExtra)
library(colorspace)

theme_set(theme_minimal())
options(digits = 5)
knitr::opts_chunk$set(
  comment = ">", echo = FALSE, warning = FALSE, fig.width = 4, fig.height = 4
)

# copied values from plots.R in pdstools, used in the custom score distrib plot
pegaClassifierBlueBar <- "#278DC1" # "dodgerblue2"
pegaClassifierYellowLine <- "goldenrod1" # "#EF8B08"
```


```{r, echo=F, warning=F, error=F, include=F}

# Code below reads the predictor data and is bloated somewhat to deal with various formatting issues,
# different product versions not always having exactly the same fields etc. In the end, it produces a
# well behaved allPredictorBins object with the current (latest) snapshot.
# If there are multiple snapshots, predPerformanceOverTime will give that data.

if (!("predictordatafile" %in% names(params))) stop("Predictor binning missing. Please provide through parameter 'predictordatafile'.")
if (!file.exists(params$predictordatafile)) stop(paste("File does not exist:", params$predictordatafile))

datamart <- ADMDatamart(modeldata = F,
                        predictordata = params$predictordatafile, # TODO: could be more protective against using model data by mistake (missing columns...)
                        filterPredictorData = function(preds) {
                          if (!is.null(params$modelid)) {
                            if (params$modelid != "") {
                              preds <- preds[ModelID == params$modelid]
                            } else {
                              if (length(unique(preds$ModelID)) > 1) {
                                stop("No model ID specified but found multiple model instances in the data.")
                              }
                            }
                          } else {
                            if (length(unique(preds$ModelID)) > 1) {
                              stop("No model ID specified but found multiple model instances in the data.")
                            }
                          }
                          if (length(unique(preds$ModelID)) != 1) {
                            stop(paste("Expected one model in the data but got", length(unique(preds$ModelID))))
                          }
                          
                          if (hasMultipleSnapshots(preds)) {
                            # Take the latest snapshots from the last day. We're doing this carefully as we don't want to report on old bins
                            # so just keeping the last day, then per predictor finding the actual last snapshot. This may not work in a situation
                            # where not all models are updated frequently.
                            
                            lastDay <- max(lubridate::floor_date(preds$SnapshotTime, unit = "days"))
                            preds <- preds[lubridate::floor_date(SnapshotTime, unit="days") == lastDay]
                            preds <- filterLatestSnapshotOnly(preds)
                          }
                          return(preds)
                        })

if (nrow(datamart$predictordata) <= 1) {
  stop(paste("No data found for model ID", params$modelid))
}

allPredictorBins <- datamart$predictordata
classifierBins <- filterClassifierOnly(allPredictorBins)
```

# Model Performance and Score Distribution

The model scores (sum of the log odds of the Naive Bayes classifier) are mapped
to propensities in the Classifier of ADM. This classifier is constructed using
the PAV (Pool Adjacent Violators) algorithm, a form of monotonic regression.

## Model Performance

The model reports a performance of **`r round(allPredictorBins[EntryType ==
"Classifier"]$Performance[1],3)`** measured as AUC-ROC. This number is
calculated from the "active" bins of the Classifier.

The "active" bins are the ones that can be reached from the current binning of
the active predictors. In the below table we give the AUC from both this active
range as well as the full range, but only the active range is really meaningful.
In a future product version, the non-reachable classifier bins may be dropped
from the table.

Next to AUC-ROC we also show the AUC of the Precision-Recall curve (AUC-PR).
This is a very different metric which can also be useful to assess the quality of
the model.

```{r, echo=F, warning=F, error=F, include=F}
activeRangeInfo <- pdstools::getActiveRanges(datamart)

# there is only 1 model now, so we can safely take the first element
classifierBins[, isInActiveRange := 
                 (BinIndex >= activeRangeInfo[[1]]$active_index_min) &
                 (BinIndex <= activeRangeInfo[[1]]$active_index_max)]

fullrange_auc_roc <- activeRangeInfo[[1]]$fullRangeAUC
fullrange_auc_pr <- pdstools::aucpr_from_bincounts(classifierBins$BinPositives, 
                                                   classifierBins$BinNegatives)

activerange_auc_roc <- activeRangeInfo[[1]]$activeRangeAUC
activerange_auc_pr <- pdstools::aucpr_from_bincounts(classifierBins[(isInActiveRange)]$BinPositives,
                                                     classifierBins[(isInActiveRange)]$BinNegatives)
```

|Total Positives|Total Negatives|Total Responses|Success Rate|
|--------------:|--------------:|--------------:|-----------------:|
|`r sum(classifierBins$BinPositives)`|`r sum(classifierBins$BinNegatives)`|`r sum(classifierBins$BinResponseCount)`|`r sprintf("%.2f%%", 100*sum(classifierBins$BinPositives)/sum(classifierBins$BinResponseCount))`|

|AUC (Reported)|AUC-ROC (Active Range)|AUC-PR (Active Range)|AUC-ROC (Full Range)|AUC-PR (Full Range)|
|-----------------:|-----------------:|-----------------:|-----------------:|-----------------:|
|**`r round(allPredictorBins[EntryType == "Classifier"]$Performance[1],5)`**|`r round(activerange_auc_roc,5)`|`r round(activerange_auc_pr,5)`|`r round(fullrange_auc_roc,5)`|`r round(fullrange_auc_pr,5)`|


```{r, results="asis", echo=F, warning=F, error=F, fig.align = "center"}
if (nrow(classifierBins) < 1) {
  cat("<p style='color:Red;'>NO data available for Classifier for date:", max(allPredictorBins$SnapshotTime), "</p>", fill=T)
}
```

## Score Distribution

The Score Distribution shows the volume and average propensity in every bin of
the score ranges of the Classifier.

The [Plotly](https://plotly.com/python/) charts have [user controls for panning,
zooming etc](https://plotly.com/chart-studio-help/zoom-pan-hover-controls/) but
note that these interactive plots do not render well in portals like Sharepoint
or Box. It is preferable to view them from a browser.

```{r fig.align="center", fig.width=8, results="asis"}
subtitle <- paste0("Performance: ", round(classifierBins$Performance[1],5), " (AUC)")

# This is a native Plotly version of the binning plot. Just 
# ggplotly(plotBinning()) works but is not as nice. Here we make sure 
# to get a clear 2nd axis, hover-overs etc.

classifierBins[, propensity := BinPositives/BinResponseCount]

#classifierBins[, isInActiveRange := (seq(.N) %% 2) == 1]

if (nrow(classifierBins) >= 1) {
  ply <- plot_ly(classifierBins) %>%
    add_bars(x = ~BinIndex, y = ~BinResponseCount,
             color = ~factor(isInActiveRange, levels=c(T, F)), 
             colors = c(pegaClassifierBlueBar, "darkgrey"),
             alpha = 0.8,
             hoverinfo = "text", 
             text = ~paste0("Score Range: ", BinSymbol, "<br>",
                            "Responses: ", BinResponseCount, "<br>",
                            "Propensity: ", sprintf("%.2f%%", 100*propensity)),
             yaxis = 'y') %>%
    add_lines(x = ~BinIndex, y = ~propensity,
              line = list(color = pegaClassifierYellowLine, width = 4),
              yaxis = 'y2') %>%
    add_markers(x = ~BinIndex, y = ~propensity,
                marker = list(color="black"),
                hoverinfo = "text", text = ~sprintf("%.2f%%", 100*propensity),
                yaxis = 'y2') %>%
    layout(title = paste0("Score Distribution of the Classifier"),
           xaxis = list(title = "Bin Index"), # to put values instead of bin indices: , tickangle = -45, tickmode = "array", tickvals = ~bin, ticktext = ~BinSymbol
           yaxis = list(side = 'left', title = "Responses"),
           yaxis2 = list(side = 'right', overlaying = "y", title = 'Propensity',
                         tickformat = '.1%', showgrid = FALSE, zeroline = FALSE, 
                         automargin = TRUE, rangemode = "tozero"),
           showlegend = FALSE,
           annotations = list(list(x = 0.5 , y = 1.02, 
                                   text = subtitle, showarrow = F, 
                                   xref='paper', yref='paper'))) %>% 
    config(displayModeBar = F)
  
  ply
}
```

Propensity is defined as $\frac{positives}{positives+negatives}$ per bin.
The adjusted propensity that is returned is a small modification (*Laplace
smoothing*) to this and calculated as
$\frac{0.5+positives}{1+positives+negatives}$ so new models initially return a
propensity of 0.5. This helps to address the cold start when introducing new
actions.

```{r, echo=F, warning=F, error=F, include=T}

## TEMP!!!
userFriendlyADMBinning <- function(bins)
{
  if (bins$EntryType[1] == "Classifier") {
    return (data.table( Index = bins$BinIndex,
                        Bin = bins$BinSymbol,
                        Positives = bins$BinPositives,
                        Negatives = bins$BinNegatives,
                        `Cum. Positives (%)` = rev(100.0*cumsum(rev(bins$BinPositives))/sum(bins$BinPositives)),
                        `Cum. Total (%)` = rev(100.0*cumsum(rev(bins$BinResponseCount))/sum(bins$BinResponseCount)),
                        `Propensity (%)` =  100*bins$BinPositives/bins$BinResponseCount,
                        `Z-Ratio` = bins$ZRatio,
                        `Lift (%)` = 100*bins$Lift,
                        `Adjusted Propensity (%)` = 100*(0.5+bins$BinPositives)/(1+bins$BinResponseCount))
    )
  } else {
    return (data.table( Index = bins$BinIndex,
                        Bin = bins$BinSymbol,
                        Positives = bins$BinPositives,
                        Negatives = bins$BinNegatives,
                        `Propensity (%)` =  100*bins$BinPositives/(bins$BinResponseCount),
                        `Z-Ratio` = bins$ZRatio,
                        `Lift (%)` = 100*bins$Lift) )
  }
}


if (nrow(classifierBins) >= 1) {
  kable(userFriendlyADMBinning(classifierBins)) %>% 
    kable_styling() %>%
    column_spec(10, bold = T)
}
```


## Cumulative Gains and Lift charts

Below are alternative ways to view the Classifier.

The Cumulative Gains chart shows the percentage of he overall cases in the "positive" category gained by targeting a percentage of the total number of cases. For example, this view shows how large a percentage of the total expected responders you target by targeting only the top decile.

The Lift chart is derived from this and shows the ratio of the cumulative gain and the targeted volume.

```{r AlternativeScoreDistributions}
activeClassifierBins <- classifierBins[(isInActiveRange)]

if (nrow(activeClassifierBins) >= 2 & (sum(activeClassifierBins$BinNegatives) + sum(activeClassifierBins$BinPositives) > 0)) {
  cumGains <- plotCumulativeGains(activeClassifierBins)
  cumLift <- plotCumulativeLift(activeClassifierBins)
} else {
  cumGains <- cumLift <- NULL
}
```


```{r, results="asis", fig.height = 3, fig.width = 6, fig.align = "center"}

# Not sure why I had to split the two chunks, but the plots did not
# render when combined in the previous chunk.
cat('<table style="width: 100%">', fill=T)
cat('<tr><td width="40%">')
ggplotly(cumGains, width=400, height=400, tooltip = c("x", "y"))
cat('</td><td width="40%">')
ggplotly(cumLift, width=400, height=400, tooltip = c("x", "y"))
cat('</td></tr></table>', fill=T)
```


# Performance by Predictor Category

Showing the performance across all predictors. The predictor categories default
to the text before the first dot. This can be customized when reading the data
for a particular customer.

```{r Predictor Category, message=FALSE, warning=FALSE}
if (any(datamart$predictordata$EntryType == "Active")) {
  plotPredictorImportance(datamart, categoryAggregateView = T) +
             # same palette as health check
             scale_fill_discrete_qualitative(palette="Dark 3", drop=T, guide="none") +
             ylab("Predictor Category") +
             scale_x_continuous(limits=c(50, NA), name="Average Univariate Performance")
} else {
  cat("No active predictors", fill=T)
}
```

# Predictor Overview

The predictors for this model are sorted by performance and grouped if they are
correlated (shown with an indentation and a lighter color).

The negatives and positives counts are usually the same across all the
predictors but will be different when predictors have been removed or added. IH
predictors often have slightly lower counts.

For Adaptive Gradient Boosting models ("AGB") the number of positives and
negatives is not available.

```{r echo=F, error=F, message=FALSE, warning=FALSE, include=T}

# TODO - consider colouring the predictor names by part before first dot ; unless there are > 10 of those

predSummary <- allPredictorBins[EntryType != "Classifier", list(Negatives = sum(BinNegatives),
                                                                Positives = sum(BinPositives),
                                                                Active = EntryType[1],
                                                                Type = Type[1],
                                                                Bins = .N,
                                                                Performance = Performance[1],
                                                                Group = GroupIndex[1]), by=PredictorName]
names(predSummary)[1] <- "Predictor"
if (nrow(predSummary) == 0) {
  cat("The model has no predictors", fill=T)
} else {
  predSummary[, maxGroupPerformance := max(Performance), by=Group]
  setorder(predSummary, -maxGroupPerformance, -Performance)
  if (uniqueN(predSummary$Group) > 1) {
    predSummary[, isFirstOfGroup := seq(.N)==1, by=Group]
  } else {
    predSummary[, isFirstOfGroup := T]
  }
  
  kable(predSummary[,-c("maxGroupPerformance", "isFirstOfGroup")]) %>%
    kable_styling() %>%
    add_indent(which(!predSummary$isFirstOfGroup)) %>%
    row_spec(row = which(!predSummary$isFirstOfGroup), color = "grey")
}
```


# Binning of the Predictors

The predictors are listed in the same order as in the summary above. Here we
show `r ifelse(params$predictordetails_activeonly, "only the active", "all")`
predictors. This can be configured with one of the parameters in the Yaml header
of the notebook that generates this report.


```{r, results="asis", fig.height = 3, fig.width = 6, fig.align = "center"}
# to print all instead of only the active ones, change condition to: EntryType != "Classifier"

for (f in unique(allPredictorBins[!params$predictordetails_activeonly | EntryType == "Active"]$PredictorName)) {
  predictorBinning <- allPredictorBins[PredictorName==f]
  
  if (nrow(predictorBinning[!is.na(Propensity)]) < 2) {
    cat("<p style='color:Red;'>NO data available for", f, "for date:", max(allPredictorBins$SnapshotTime), "</p>", fill=T)
  } else {
    cat("##", f, fill=T) # Predictor name
    
    cat('<table style="width: 100%"><tr><th>',params$modeldescription,'</th><th>Binning</th></tr>', fill=T)
    
    # write.csv(predictorBinning, "tempbin.csv")
    
    # Find other predictors in the same group
    correlatedPreds <- predSummary[Group != 0 & Group == predSummary[Predictor==f]$Group & Predictor != f]
    if (nrow(correlatedPreds) > 0) { 
      extraPredInfo <- list("Correlated Predictors" = paste(sort(correlatedPreds$Predictor), collapse = ", "))
    } else {
      extraPredInfo <- list("Correlated Predictors" = "--")
    }
    
    # Table prelude with some overall info about the predictor
    cat('<tr><td width="40%">', fill=T)
    printADMPredictorInfo(f, predictorBinning, extraPredInfo)
    cat("</td>", fill=T)
    
    # colour names: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
    
    cat('<td width="60%">', fill=T)
    if (nrow(predictorBinning) > 1) {
      p <- plotBinning(predictorBinning)+ ggtitle(f)
      print(p)
      
      p <- plotBinningLift(predictorBinning)+ ggtitle(f)
      print(p)
    }  
    cat("</td></tr></table>", fill=T)
    
    print(kable(userFriendlyADMBinning(predictorBinning), format = "markdown"))
  }
}
```




