---
title: "`r params$title`"
subtitle: "`r params$modeldescription`"
author: "Pega CDH Tools"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
always_allow_html: true
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set. That also allows to run the notebook multiple times for different values.
  
  title:
    value: "Adaptive Model Report"
    
  predictordatafile:
    # Full path to ADM predictor binning table data. There is 
    # flexibility in format: CSV, zipped CSV or dataset export. If you have something
    # different, you'll need to load, filter and re-write as CSV first.
    value: "../../data/pr_data_dm_admmart_pred.csv"
    
  modeldescription:
    # Optional model description typically corresponding to the model info of the indicated model
    value: "Sales Model - PSDISCOUNT100"
    
  modelid:
    # Model ID that is used to filter the predictor binning table data. If the
    # binning data only contains a single Model ID it can be left empty.
    value: "68e1d164-81e3-5da0-816c-0bbc3c20ac6c" # default
    # value: "N/A"
---

```{r, echo=F, warning=F, error=F, include=FALSE}
library(cdhtools)

# include below when developing the library
sapply(list.files("~/Documents/pega/cdh-datascientist-tools/r/R", "*.R", full.names = T), source)

library(data.table)
library(lubridate)
# library(ggplot2)
library(scales)
library(knitr)
library(kableExtra)
library(plotly)
# library(readxl) - standard support for Excel is a bit tricky, we recommend saving as CSV or read/write in a different R/Python script first
library(gridExtra)

theme_set(theme_minimal())
options(digits = 5)
knitr::opts_chunk$set(
  comment = ">", echo = FALSE, warning = FALSE, fig.width = 4, fig.height = 4
)
```


```{r, echo=F, warning=F, error=F, include=F}

# Code below reads the predictor data and is bloated somewhat to deal with various formatting issues,
# different product versions not always having exactly the same fields etc. In the end, it produces a
# well behaved modelPredictorBins object with the current (latest) snapshot.
# If there are multiple snapshots, predPerformanceOverTime will give that data.

if (!("predictordatafile" %in% names(params))) stop("Predictor binning missing. Please provide through parameter 'predictordatafile'.")
if (!file.exists(params$predictordatafile)) stop(paste("File does not exist:", params$predictordatafile))

# This read function is a helper for theses off-line reports. It looks at file
# extensions to figure out whether to read CSV, a model export zip or something
# else, then also sanitizes and standardizes the output. For specific needs we
# recommend using readDSExport. If you need to deal with CSV's read with fread then 
# standardize the casing with "applyUniformPegaFieldCasing" and fix up the
# date/time fields if necessary.
modelPredictorBins <- readDatamartFromFile(params$predictordatafile)

# make sure there is only ONE model ID or subset to just the one passed in
if (!(is.null(params$modelid) || (params$modelid %in% c("", "NA", "N/A", "na", "n/a")))) {
  modelPredictorBins <- modelPredictorBins[ModelID == params$modelid]
  if (nrow(modelPredictorBins) <= 1) {
    stop(paste("No data found for model ID", params$modelid))
  }
} else {
  if (length(unique(modelPredictorBins$ModelID)) > 1) {
    stop(paste0("Expected only a single model ID in the data, got ", 
                length(unique(modelPredictorBins$ModelID)), ". Pass in a model ID or split the file."))
  }
}

# # in older versions "PredictorType" was called "Type" - keep both
# if("Type" %in% names(modelPredictorBins) & !"PredictorType" %in% names(modelPredictorBins)) {
#   modelPredictorBins[, PredictorType := Type]
# }
# if(!"Type" %in% names(modelPredictorBins) & "PredictorType" %in% names(modelPredictorBins)) {
#   modelPredictorBins[, Type := PredictorType]
# }

# check for presence of required fields
requiredFields <- c("SnapshotTime","ModelID",
                    "PredictorName","PredictorType","Type","Performance",
                    "BinIndex","BinSymbol","BinNegatives","BinPositives","EntryType","ZRatio","Lift")
requiredFieldsForActualPerformance <- c("BinType", "BinLowerBound","BinUpperBound")
optionalFields <- c("GroupIndex", requiredFieldsForActualPerformance) # not present in all product versions

if (!all(sapply(requiredFields, function(x) { return(x %in% names(modelPredictorBins)) }))) {
  stop(paste("Not all required fields present. Expected:", paste(requiredFields, collapse = ", "), 
             "\ngot:", paste(names(modelPredictorBins), collapse = ", "),
             "\nmissing:", paste(setdiff(requiredFields, names(modelPredictorBins)) , collapse = ", ")))
}

# keep only the required + optional fields in the data so to avoid implicit assumptions
modelPredictorBins <- modelPredictorBins[, intersect(names(modelPredictorBins), c(requiredFields, optionalFields)), with=F]

# Predictor binning can have multiple snapshots. Keeping performance over time but only the last binning.
hasMultipleSnapshots <- (length(unique(modelPredictorBins$SnapshotTime)) > 1)

if (hasMultipleSnapshots) {
  predPerformanceOverTime <- unique(modelPredictorBins[, c("PredictorName", "Performance", "SnapshotTime"), with=F])  
  
  # Take the latest snapshots from the last day. We're doing this carefully as we don't want to report on old bins
  # so just keeping the last day, then per predictor finding the actual last snapshot. This may not work in a situation
  # where not all models are updated frequently.
  
  # # # NB we have seen situations where other formats appeared after import/export to Excel - may need to deal w that a la as.POSIXct(strptime(SnapshotTime, format="%Y-%m-%d"))
  # modelPredictorBins[, snapshot := fromPRPCDateTime(SnapshotTime)] 
  # if (sum(is.na(modelPredictorBins$snapshot))/nrow(modelPredictorBins) > 0.2) {
  #   modelPredictorBins[, snapshot := parse_date_time(SnapshotTime, orders=c("%Y-%m-%d %H:%M:%S"))] 
  #   if (sum(is.na(modelPredictorBins$snapshot))/nrow(modelPredictorBins) > 0.2) {
  #     stop("Assumed Pega date-time string but resulting in over 20% NA's in snapshot time after conversion. Check that this is valid or update the code that deals with date/time conversion.")
  #   }
  # }
  
  lastDay <- max(lubridate::floor_date(modelPredictorBins$SnapshotTime, unit = "days"))
  modelPredictorBins <- modelPredictorBins[lubridate::floor_date(SnapshotTime, unit="days") == lastDay]
  modelPredictorBins <- modelPredictorBins[, .SD[SnapshotTime == max(SnapshotTime)], by=c("ModelID")]
}

# recalculate a few fields that are used - use the naming conventions from the data mart
# NB Performance, Z-Ratio, Lift could have been calculated from the bins but not doing so guarantees consistency with the product reports

modelPredictorBins[, BinResponseCount := (BinPositives+BinNegatives)]
modelPredictorBins[, Positives := sum(BinPositives), by=PredictorName]
modelPredictorBins[, Negatives := sum(BinNegatives), by=PredictorName]

# predictor grouping index was not always there, add it as just a sequence number when absent
if (!("GroupIndex" %in% names(modelPredictorBins))) {
  modelPredictorBins[, GroupIndex := .GRP, by=PredictorName]
}

setorder(modelPredictorBins, -Performance, BinIndex)
```

# Model Performance and Score Distribution

The model scores (sum of the log odds of the Naive Bayes classifier) are mapped to propensities in the Classifier of ADM. This classifier is constructed using the PAV (Pool Adjacent Violators) algorithm, a form of monotonic regression.

## Model Performance

The model reports a performance of `r round(modelPredictorBins[EntryType == "Classifier"]$Performance[1],5)` measured in AUC. If supporting data is available, the actual AUC is recalculated using only those bins that fall into the current score range (if available, shown in parentheses in the title and bins not in range greyed out).

```{r, echo=F, warning=F, error=F}
classifierBinning <- modelPredictorBins[EntryType == "Classifier"]

actualPerformance <- NULL
classifierBinning[, inCurrentRange := T]

# TODO: commented out because it depends heavily on names of fields that have recently changed
# if (all(sapply(requiredFieldsForActualPerformance, function(f) {return(f %in% names(modelPredictorBins))}))) {
#   # TODO consider try/catch because this one easily fails
#   actualPerformance <- getModelPerformanceOverview(dmPredictors = modelPredictorBins)
#   classifierBinning[, inCurrentRange := (BinIndex >= actualPerformance$actual_score_bin_min & BinIndex <= actualPerformance$actual_score_bin_max)]
# }
```

|Total Positives|Total Negatives|Total Responses|Overall Propensity|
|--------------:|--------------:|--------------:|-----------------:|
|`r sum(classifierBinning$BinPositives)`|`r sum(classifierBinning$BinNegatives)`|`r sum(classifierBinning$BinResponseCount)`|`r sprintf("%.2f%%", 100*sum(classifierBinning$BinPositives)/sum(classifierBinning$BinResponseCount))`|

```{r, results="asis", echo=F, warning=F, error=F, fig.align = "center"}
if (nrow(classifierBinning) < 1) {
  cat("<p style='color:Red;'>NO data available for Classifier for date:", max(modelPredictorBins$SnapshotTime), "</p>", fill=T)
}
```

## Cumulative Gains and Lift charts

Below are alternative ways to view the Classifier.

The Cumulative Gains chart shows the percentage of he overall cases in the "positive" category gained by targeting a percentage of the total number of cases. For example, this view shows how large a percentage of the total expected responders you target by targeting only the top decile.

The Lift chart is derived from this and shows the ratio of the cumulative gain and the targeted volume.

```{r, fig.align = "left", fig.width=8, fig.height=3}
# right align is nicer but plotly doesnt do that

# for inspiration on the charts:
# see http://dmg.org/pmml/v4-0-1/ModelExplanation.html#gainscharts
# and https://www.ibm.com/support/knowledgecenter/de/SSLVMB_24.0.0/spss/tutorials/mlp_bankloan_outputtype_02.html

subtitle <- paste0("Performance: ", round(classifierBinning$Performance[1],5), " (AUC)")
if (!is.null(actualPerformance)) {
  if (round(classifierBinning$Performance[1],5) != round(classifierBinning$Performance[1],5)) {
    subtitle <- paste0("Performance: ", round(classifierBinning$Performance[1],5), 
                       " (AUC) (actual: ", round(actualPerformance$actual_performance,5),")")
  }
}

if (nrow(classifierBinning) >= 1 & (sum(classifierBinning$BinNegatives) + sum(classifierBinning$BinPositives) > 0)) {
  cumGains <- plotADMCumulativeGains(classifierBinning)
  
  cumLift <- plotADMCumulativeLift(classifierBinning)
  
  # TODO: add a cum Success rate one. Like lift but w/o the division.
  # TODO: consider ggplot of these, those work just fine
  # TODO: classic one on separate line. Make bars less prominent. Swap y-axes - they're opposite what product does.
  
  classic <- plotADMBinning(classifierBinning) + ggtitle("Score Distribution")
  
  #ggplotly(cumGains) - TODO convert to plotly plot as ggplotly doesnt show both axes

  grid.arrange(cumGains, cumLift, classic, nrow=1)
}
```

## Score Distribution

The Score Distribution shows the volume and average success rate in every bin of the score ranges of the Classifier. Same plot as before but
this is an interactive one, using Plotly.

```{r fig.align="center", fig.width=8, results="asis"}

# Using a native Plotly version of the graph to make the hover-over work. Just 
# plotlygg(p) doesnt work perfectly, unfortunately.

classifierBinning[, propensity := 100*BinPositives/BinResponseCount]

pegaClassifierBlueBar <- "#278DC1"
pegaClassifierYellowLine <- "EF8B08"

if (nrow(classifierBinning) >= 1) {
  ply <- plot_ly(classifierBinning) %>%
    add_bars(x = ~BinIndex, y = ~BinResponseCount, 
             color = ~factor(inCurrentRange, levels=c(T, F)), 
             colors = c(pegaClassifierBlueBar, "darkgrey"),
             alpha = 0.8,
             hoverinfo = "text", 
             text = ~paste0("Score Range: ", BinSymbol, "\nResponses: ", BinResponseCount, "\nSuccess Rate: ", sprintf("%.2f%%", propensity)),
             yaxis = 'y') %>%
    add_lines(x = ~BinIndex, y = ~propensity,
              line = list(color = pegaClassifierYellowLine, width = 4),
              yaxis = 'y2') %>%
    add_markers(x = ~BinIndex[(classifierBinning$inCurrentRange)], y = ~propensity[(classifierBinning$inCurrentRange)],
                marker = list(color="black"),
                hoverinfo = "text", text = ~sprintf("%.2f%%", propensity[(classifierBinning$inCurrentRange)]),
                yaxis = 'y2') %>%
    # outside the current range
    add_markers(x = ~BinIndex[(!classifierBinning$inCurrentRange)], y = ~propensity[(!classifierBinning$inCurrentRange)],
                marker = list(color="darkgrey"),
                hoverinfo = "text", text = ~sprintf("%.2f%%", propensity[(!classifierBinning$inCurrentRange)]),
                yaxis = 'y2') %>%
    layout(title = paste0("Score Distribution of the Classifier"),
           xaxis = list(title = ""), # to put values instead of bin indices: , tickangle = -45, tickmode = "array", tickvals = ~bin, ticktext = ~BinSymbol
           yaxis = list(side = 'left', title = "Responses"),
           yaxis2 = list(side = 'right', overlaying = "y", title = 'Propensity (%)', 
                         showgrid = FALSE, zeroline = FALSE, automargin = TRUE, rangemode = "tozero"),
           showlegend = FALSE,
           annotations = list(list(x = 0.5 , y = 1.02, 
                                   text = subtitle, showarrow = F, 
                                   xref='paper', yref='paper'))) %>% 
    config(displayModeBar = F)
  
  ply
}
```

Propensity is defined as $\frac{positives}{positives+negatives}$ per bin. 

The adjusted propensity that is returned is a small modification (Laplace smoothing) to this and calculated as $\frac{0.5+positives}{1+positives+negatives}$ so empty models return a propensity of 0.5.

```{r, echo=F, warning=F, error=F, include=T}
if (nrow(classifierBinning) >= 1) {
  kable(userFriendlyADMBinning(classifierBinning)) %>% kable_styling()
}
```

# Performance by Predictor Category

Active predictors grouped by category.

```{r Predictor Category}
plotADMPredictorImportance(modelPredictorBins, categoryAggregateView = T) +
  scale_fill_discrete(guide="none")
```

# Predictor Overview

Number of positives and negatives in each bin and the derived lift and Z-ratio. If grouping information is available, strongly correlated predictors are grouped, with the highest performance predictor groups on top. Groups are indicated by indentation.

```{r echo=F, error=F, include=T}

# TODO - the grouping could be displayed in more fancy ways using kableExtra options for grouping
# TODO - consider colouring the predictor names by part before first dot ; unless there are > 10 of those

predSummary <- modelPredictorBins[EntryType != "Classifier", .(Negatives = sum(BinNegatives),
                                                               Positives = sum(BinPositives),
                                                               Active = EntryType[1],
                                                               Type = PredictorType[1],
                                                               Bins = .N,
                                                               Performance = Performance[1],
                                                               Group = GroupIndex[1]), by=PredictorName]
names(predSummary)[1] <- "Predictor"
if (nrow(predSummary) == 0) {
  cat("The model has no predictors", fill=T)
} else {
  predSummary[, maxGroupPerformance := max(Performance), by=Group]
  setorder(predSummary, -maxGroupPerformance, -Performance)
  predSummary[, isFirstOfGroup := seq(.N)==1, by=Group]
  
  kable(predSummary[,-c("maxGroupPerformance", "isFirstOfGroup")]) %>%
    kable_styling() %>%
    add_indent(which(!predSummary$isFirstOfGroup))
}
```


# Binning of the active Predictors

Binning of all individual predictors. Predictors are listed in the same order as in the summary above.

```{r, results="asis", fig.height = 4, fig.width = 6, fig.align = "center"}

# to print all instead of only the active ones, change condition to: EntryType != "Classifier"

for (f in unique(modelPredictorBins[EntryType == "Active"]$PredictorName)) {
  predictorBinning <- modelPredictorBins[PredictorName==f]

  if (nrow(predictorBinning) < 1) {
    cat("<p style='color:Red;'>NO data available for", f, "for date:", max(modelPredictorBins$SnapshotTime), "</p>", fill=T)
  } else {
    
    # Find other predictors in the same group
    correlatedPreds <- predSummary[Group == predSummary[Predictor==f]$Group & Predictor != f]
    if (nrow(correlatedPreds) > 0) { 
      extraPredInfo <- list("Correlated Predictors" = paste(sort(correlatedPreds$Predictor), collapse = ", "))
    } else {
      extraPredInfo <- list("Correlated Predictors" = "--")
    }
    
    # Table prelude with some overall info about the predictor
    printADMPredictorInfo(f, predictorBinning, extraPredInfo)

    # colour names: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
    
    if (nrow(predictorBinning) > 1) {
      p <- plotADMBinning(predictorBinning)+ ggtitle(f)
      
      print(p)
    }  
    
    print(kable(userFriendlyADMBinning(predictorBinning), format = "markdown"))
  }
}
```

