---
title: "`r params$title`"
subtitle: "`r params$subtitle`"
author: "Pega CDH Tools"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
always_allow_html: true
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set, although you can also run it from R Studio: Knit with Parameters to be prompted 
  # for the paths to the data.
  
  title:
    value: "ADM Health Check"
    
  subtitle:
    value: ""
    
  modelfile:
    # Full path to the source file which should be an export of the ADM model table
    # can be a plain CSV file, a zipped CSV, or the full path of a dataset export
    value: "../../data/pr_data_dm_admmart_mdl_fact.csv"

  predictordatafile:
    # Optional full path to ADM predictor binning table data. If given, the model 
    # overview will also contain predictor plots. Like the model file, there is 
    # flexibility in format: CSV, zipped CSV or dataset export. If you have something
    # different, you'll need to load, filter and re-write as CSV first.
    value: "../../data/pr_data_dm_admmart_pred.csv"

  modellist:
    # Facilitates automatic generation of individual model reports (calling the
    # off-line model report notebook). Optional. When given, is the name of a 
    # text file that will be generated from the model data with a list of 
    # model ID's (and some other info). This file can be iterated
    # over in a batch script to generate individual model reports.
    value: ""
  
  # Number of propositions, models and predictors to put in lists and plots
  max_propositions_in_plot:
    value: 20
  max_models_in_list:
    value: 50
  max_predictors_in_plot:
    value: 30
---


```{r, echo=F, warning=F, error=F, include=FALSE}
library(cdhtools)

# include below when developing the library
# sapply(list.files("~/Documents/pega/cdh-datascientist-tools/r/R", "*.R", full.names = T), source)

library(data.table)
library(lubridate)
library(ggplot2)
library(plotly)
library(colorspace)
library(scales)
library(knitr)
library(kableExtra)
library(stringi)
library(arrow) # for parquet read/write

theme_set(theme_light())
options(digits = 5)
knitr::opts_chunk$set(
  comment = ">", echo = FALSE, warning = FALSE, fig.width = 10, fig.height = 10
)
```

This notebook gives a generic and global overview of the Adaptive Models 
and predictors.

The goals is to provide a first-level scan of the models after which a 
deeper and more customer specific deeper dive can be done.

Some of the plots provide some interactivity. For best viewing results, open the 
HTML in a browser. Viewing the HTML from platforms like e.g. Sharepoint or 
Github will loose the interactive charts. However as a backup, non-interactive, 
static, plots are also provided.

The default notebook output is a single-page HTML. You can also export to PDF 
as well as other formats supported by Pandoc (e.g. Word).

To create stand-alone model reports for individual model instances see the 
*modelreport* notebook.

```{r Read Model Data, echo=F, error=F, warning=FALSE, include=F}

if (!("modelfile" %in% names(params))) stop("Model data missing. Please provide through parameter 'modelfile'.")
if (!file.exists(params$modelfile)) stop(paste("File does not exist:", params$modelfile))

if (params$predictordatafile == "") params$predictordatafile <- NULL
if (!is.null(params$predictordatafile)) {
  if (!file.exists(params$predictordatafile)) stop(paste("File does not exist:", params$predictordatafile))  
}

datamart <- ADMDatamart(params$modelfile, params$predictordatafile)

# standardized coloring for this script
# see also https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf
scale_color_Channel <- scale_colour_brewer(limits=levels(datamart$modeldata$Channel), 
                                           name="Channel", palette="Set1", drop=T)
scale_fill_Channel <- scale_fill_brewer(limits=levels(datamart$modeldata$Channel), 
                                        name="Channel", palette="Set1", drop=T)
scale_fill_PredictorCategory <- scale_fill_discrete_qualitative(limits=levels(datamart$predictordata$PredictorCategory), 
                                                                name="Predictor Category", drop=T)

# myColors <- brewer.pal(3, "Spectral")
# names(myColors) <- levels(iris$Species)
# custom_colors <- scale_colour_manual(name = "Species Names", values = myColors)
# 
# scale_colour_hue(limits=levels(data$type),drop=TRUE)

# Use qualitative for categorical data. To show the palettes (qualitative in the middle):
# library(RColorBrewer)
# par(mar=c(3,4,2,2))
# display.brewer.all()
```

# Overview of the Propositions

Propositions, also referred to as offers or actions with treatments, are
the most granular perspective on the adaptive models.

## Proposition Success Rates across Channels

Showing the current success rate of the propositions. Different channels usually 
have very different success rates. Just showing the top `r params$max_propositions_in_plot`
here and limiting to the propositions that have received at least 100 responses. 

Look out for propositions that stand out, having a far higher success rate
than the rest. Check with business if that is expected.

```{r Plot Proposition Success Rates}
p <- plotPropositionSuccessRates(datamart, 
                                 filter=function(mdls){ filterLatestSnapshotOnly(mdls)[ResponseCount >= 100] }, 
                                 limit=params$max_propositions_in_plot, 
                                 facets="Channel") + 
  scale_fill_continuous_divergingx()

# ggplotly(p) %>% layout(showlegend=FALSE) # plotly no use here given the way the plot is constructed
p
```

## Proposition Success Rates over Time

Showing how the proposition success rates evolved over time. Again split by
Channel but now also by model configuration. Usually there are separate model
configurations for different channels but sometimes there are also additional
model configurations for different outcomes (e.g. conversion) or different
customers (e.g. anonymous).

```{r Success Rate over Time, message=FALSE, warning=FALSE}
if (!hasMultipleSnapshots(datamart$modeldata))
{
  cat("Trend plots will only be available when the model data contains multiple snapshots.", fill=T)
} else {
  p <- plotSuccessRateOverTime(datamart, 
                               filter=function(mdls){ mdls[ResponseCount >= 100] }, 
                               aggregation = "Channel") + 
    scale_color_Channel
  #geom_smooth(aes(fill=Channel), alpha=0.1)
  p
}
```


# Overview of Adaptive Models

There are a total of `r nrow(filterLatestSnapshotOnly(datamart$modeldata))` models 
in the latest snapshot.

In the sections below we check which of these models have reached certain
reliability (or "maturity") thresholds. This is based on heuristics on both
the number of positives (> 200 considered mature) and performance.

We then list the models that don't match one of the criteria, limiting to
the first `r params$max_models_in_list`.

## Empty and Immature Models

```{r CategorizeModels}
models4MaturityChecks <- datamart$modeldata

models4MaturityChecks[, isEmpty := ResponseCount == 0]
models4MaturityChecks[, noPositives := ResponseCount > 0 & Positives == 0]
models4MaturityChecks[, isImmature := Positives > 0 & Positives < 200]
models4MaturityChecks[, noPerformance := Positives >= 200 & Performance == 0.5]

availableStandardContextKeys <- intersect(c("ConfigurationName", "Issue","Group","Name","Channel","Direction","Treatment"), names(models4MaturityChecks))
fieldsForModelAnalysis <- c(availableStandardContextKeys, c("ResponseCount", "Positives", "Negatives", "Performance"))

setorderv(models4MaturityChecks, availableStandardContextKeys)

modelsNeverUsed <- filterLatestSnapshotOnly(models4MaturityChecks)[(isEmpty), fieldsForModelAnalysis, with=F]
modelsNoPositives <- filterLatestSnapshotOnly(models4MaturityChecks)[(noPositives), fieldsForModelAnalysis, with=F]
modelsImmature <- filterLatestSnapshotOnly(models4MaturityChecks)[(isImmature), fieldsForModelAnalysis, with=F]
modelsNoPerformance <- filterLatestSnapshotOnly(models4MaturityChecks)[(noPerformance), fieldsForModelAnalysis, with=F]
```

### Models that have never been used

These models have no responses at all: no positives but also no negatives. 

Showing first `r min(nrow(modelsNeverUsed), params$max_models_in_list)` from total of `r nrow(modelsNeverUsed)`.

```{r}
modelsNeverUsed %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>% 
  scroll_box(height = "300px")
```

### Models that have have been used but never received a positive response

These models have been used but never received a "positive" response.

Showing first `r min(nrow(modelsNoPositives), params$max_models_in_list)` from total of `r nrow(modelsNoPositives)`.

```{r}
modelsNoPositives %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>% 
  scroll_box(height = "300px")
```


### Models that are still in an immature phase of learning

These models have received at least one positive response but not enough to
be qualified to be fully "mature" - a concept that matters especially for 
outbound channels.

Showing first `r min(nrow(modelsImmature), params$max_models_in_list)` from total of `r nrow(modelsImmature)`.

```{r}
modelsImmature %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>% 
  scroll_box(height = "300px")
```


### Models that have received sufficient responses but are still at their minimum performance

These models also have received at over 200 positives but still show the minimum
model performance. This could be an indication of data problems, or not having the
right predictors but may also be caused by technical aspects like the order of
the responses to the model.

Showing first `r min(nrow(modelsNoPerformance), params$max_models_in_list)` from total of `r nrow(modelsNoPerformance)`.

```{r}
modelsNoPerformance %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>% 
  scroll_box(height = "300px")
```


### Number of Empty/Immature Models over time

The number of empty or immature models over time. Empty is defined as having
no responses at all. Immature is defined as having < 200 positives, and no
performance means model performance is still the initial 0.5 value while having
matured already according to the definition.

```{r Number of Empty or Immature models over time, message=FALSE, warning=FALSE}
if (!hasMultipleSnapshots(datamart$modeldata))
{
  cat("Trend plots will only be available when the model data contains multiple snapshots.", fill=T)
} else {
  
  immatureModelsByChannel <- models4MaturityChecks[, list(`Empty Models` = length(unique(ModelID[(isEmpty)])), 
                                                          `Models w/o Positives` = length(unique(ModelID[(noPositives)])), 
                                                          `Immature Models` = length(unique(ModelID[(isImmature)])), 
                                                          `Models w/o Performance` = length(unique(ModelID[(noPerformance)])),
                                                          `Number of non-empty Models` = length(unique(ModelID[(!isEmpty)]))), 
                                                   by=c("Channel", "SnapshotTime")]
  
  ggplot(melt(immatureModelsByChannel, c("Channel", "SnapshotTime"), 
              variable.name="Count", value.name="N"),
         aes(SnapshotTime, N, color=Count)) + 
    geom_line(size=1) + xlab("") + ylab("Count") +
    scale_color_discrete_qualitative() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    facet_wrap(. ~ Channel) +
    ggtitle("Immature and Empty Models", subtitle = "by Channel")
}
```

## Model Performance

### Model Performance vs Proposition Success Rates (the Bubble Chart)

This "Bubble Chart" - similar to the standard plot in the ADM reports in Pega - 
shows the relation between model performance and proposition success rates. In 
addition, the size of the bubbles indicates the number of responses.

This view can help to identify several types of issues.

If all the bubbles clutter too much on the left-hand side of the charts, this
means the models are not predictive (yet). Bubbles at the bottom of the charts
represent propositions with very low success rates. In an ideal scenario you
will see the larger bubbles more on the top-right, so more volume for propositions
with higher success rates and better models.

These bubble charts can lead to several actions, e.g.

* See if model performance can be increased by adding new predictors.

* See if the engagement rules in the Decision Strategy are overly restrictive.

* Reconsider the arbitration of the propositions so they get more (or less)
exposure.

Etc.


```{r Bubble Chart, message=FALSE, warning=FALSE}
plt <- plotPerformanceSuccessRateBubbleChart(datamart, 
                                             filter=function(mdls){ filterLatestSnapshotOnly(mdls)[ResponseCount >= 100] }, 
                                             facets = c("ConfigurationName", "Channel"))

ggplotly(plt) %>% 
  layout(showlegend=F) 
```

Static version for viewing this on platforms that do not 
support the interactive Plotly graphs.

Instead of using a color for every proposition, here we use color to indicate how
"good" the models are: green for models with higher number of positives and 
higher model performance.

```{r}
plt <- plotPerformanceSuccessRateBubbleChart(datamart, 
                                             filter = function(mdls)
                                             { 
                                               m <- filterLatestSnapshotOnly(mdls)[ResponseCount >= 100] 
                                               m[, OhMyGoodness := (cdhtools::auc2GINI(Performance) + pmin(Positives, 200) * (1.0/200))/2]
                                               return(m)
                                             }, 
                                             facets = c("ConfigurationName", "Channel"),
                                             color = "OhMyGoodness") +
  scale_colour_gradient(low="darkred", high="darkgreen")

plt
```

### Model Performance over Time

Showing how the model performance evolves over time.

Aggregating up to Channel and splitting by model configuration.

```{r Performance over Time, message=FALSE, warning=FALSE}
if (!hasMultipleSnapshots(datamart$modeldata))
{
  cat("Trend plots will only be available when the model data contains multiple snapshots.", fill=T)
} else {
  p <- plotPerformanceOverTime(datamart, aggregation = "Channel") +
    scale_color_Channel  
  #geom_smooth(aes(fill=Channel), alpha=0.1)
  p
  # ggplotly(p)
}
```


## Which Models drive most of the Volume

### Analysis of skewness of the Responses

Showing the cumulative response count vs the number of models. 

If this line strongly deviates from the diagonal it means that relatively few
models are responsible to drive the majority of the responses. Note that in 
this plot we look at the total responses, which really means that we are looking
at "impressions" mostly. 

Too much skewness may be a reason to check in with business and verify that
this is expected.

```{r fig.height=5, fig.width=8}
responseGainData <- 
  filterLatestSnapshotOnly(datamart$modeldata) [, list(Responses = max(ResponseCount)), 
                                                by=c("Channel", "ModelID")][order(Channel,Responses,decreasing = T)]
responseGainData[ , TotalResponseFraction := cumsum(Responses) / sum(Responses) , by=Channel]
responseGainData[ , TotalModelsFraction := seq(.N)/.N , by=Channel]

plt <- ggplot(responseGainData, aes(TotalModelsFraction, TotalResponseFraction, color=Channel)) + 
  geom_line(size=1) +
  scale_color_Channel +
  scale_x_continuous(name="Percentage of Models", labels = scales::percent, limits = c(0,1)) +
  scale_y_continuous(name="Percentage of Responses", labels = scales::percent, limits = c(0,1)) +
  #geom_abline(slope=1, intercept=0, color="grey", linetype="dashed") +
  ggtitle("Cumulative Responses by Models", subtitle = "by Channel")

# ggplotly(plt)
plt
```


### Models with largest number of positive responses.

Similar to the above plots here we list the models that received the
most positive responses, or responses in general.

```{r fig.height=8, fig.width=8}
topNPositives <- filterLatestSnapshotOnly(datamart$modeldata) [frank(-Positives, ties.method="dense") <= params$max_propositions_in_plot][order(-Positives)]
topNPositives[, Label := apply(topNPositives[,availableStandardContextKeys,with=F], 1, paste, collapse="/")]
topNPositives[, Rank := factor(seq(.N), levels=rev(seq(.N)))]

ggplot(topNPositives,
       aes(Rank, Positives, fill=Channel)) +
  geom_col() +
  geom_text(aes(y=0,label=Label), size=3, hjust=0) +
  coord_flip() +
  scale_fill_Channel +
  ggtitle(paste("Top", params$max_propositions_in_plot, "highest Positives"))
```

### Models with largest number of responses (positive or negative).

```{r fig.height=8, fig.width=8}
topNResponses <- filterLatestSnapshotOnly(datamart$modeldata) [frank(-ResponseCount, ties.method="dense") <= params$max_propositions_in_plot][order(-ResponseCount)]
topNResponses[, Label := apply(topNResponses[,availableStandardContextKeys,with=F], 1, paste, collapse="/")]
topNResponses[, Rank := factor(seq(.N), levels=rev(seq(.N)))]

ggplot(topNResponses,
       aes(Rank, ResponseCount, fill=Channel)) +
  geom_col() +
  geom_text(aes(y=0,label=Label), size=3, hjust=0) +
  coord_flip() +
  scale_fill_Channel +
  ggtitle(paste("Top", params$max_propositions_in_plot, "highest Responses"))
```


### Analysis of Performance vs Volume

An alternative view on the relation between model performance and volume
is given below. Model performance is on the horizontal axis and volume
on the vertical.

Ideally, more volume is driven by more mature models.

Showing the response count vs. the model performance (AUC).

```{r fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
cumAUCOverVolumeData <- 
  filterLatestSnapshotOnly(datamart$modeldata) [, list(Performance = max(Performance), 
                                                       ResponseCount = max(ResponseCount)), 
                                                by=c("Channel", "ModelID")]
cumAUCOverVolumeData[, TotalResponses := sum(ResponseCount), by=Channel]
cumAUCOverVolumeData[, PerformanceBin := cut(Performance, breaks=15)]

plt <- ggplot(cumAUCOverVolumeData[, list(N = sum(ResponseCount)/TotalResponses), 
                                   by=c("PerformanceBin", "Channel")], 
              aes(PerformanceBin, N, color=Channel)) + 
  #geom_col(position = position_dodge()) +
  geom_smooth(aes(group=Channel), se = F) + 
  scale_color_Channel +
  scale_y_continuous(name="Percentage of Responses", labels = scales::percent) +
  xlab("Model Performance") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  ggtitle("Distribution of Volume vs Model Performance", subtitle = "by Channel")

ggplotly(plt)
```

Static version for viewing this on platforms that do not 
support the interactive Plotly graphs.

```{r fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
plt
```

### Positives vs. Number of Models

In this view we look at the number of models vs the number of positives. The goal
here is simply to check that most models have a significant number of positives.

Ideally there are only few models without positives and the vast majority
is on the right-hand side of the plot, with > 200 positives.

```{r fig.height=5, fig.width=8, message=FALSE}
modelsByPositives <- filterLatestSnapshotOnly(datamart$modeldata)
modelsByPositives[, PositivesBin := cut(Positives, breaks=c(seq(0,200,by=10), Inf), 
                                        right=F)]
modelsByPositives <- modelsByPositives[, list(Positives = min(Positives), 
                                              Models = uniqueN(ModelID)),
                                       by=c("Channel", "PositivesBin")]
setorder(modelsByPositives, Positives)
modelsByPositives[, cumModels := Models/sum(Models), by="Channel"]

plt <- ggplot(modelsByPositives, aes(PositivesBin, cumModels, color=Channel)) + 
  geom_line(aes(group=Channel)) + geom_point() +
  scale_color_Channel +
  scale_y_continuous(name="Percentage of Models", labels = scales::percent) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  xlab("Positives") +
  ggtitle("Percentage of models vs number of positive responses", subtitle = "by Channel")

ggplotly(plt)
```

Static version for viewing this on platforms that do not 
support the interactive Plotly graphs.

```{r fig.height=5, fig.width=8, message=FALSE}
plt
```



## Propensity distribution

Analysing the volume of responses vs the propensity range. In a more
emphathetic setup, you would expect higher volume for higher propensities. Often
however, multiple factors are included in the prioritization, changing this
picture.

Note that the propensity bins are not of equal width. Propensities are typically
very low so with an equal width distribution, almost all volume would be in the
first bins. The binning here is based on (roughly) equal volume across all data.

So when one of the graphs shows more volume on the left, that is to be 
interpreted as relative to the other graphs.

```{r Propensity distribution, message=FALSE, warning=FALSE}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  classifiers <- merge(filterClassifierOnly(datamart$predictordata), 
                       unique(datamart$modeldata[,c("ModelID","Channel")]), by="ModelID")
  # classifiers[, Propensity := (0.5+BinPositives)/(1+BinResponseCount)]
  classifiers[, PropensityRange := Hmisc::cut2(Propensity, g=15)]
  classifiers[, TotalVolume := sum(BinResponseCount), by="Channel"]
  
  ggplot(classifiers[,list(RelVolume = sum(BinResponseCount)/TotalVolume, 
                           Propensity = min(Propensity)), 
                     by=c("Channel", "PropensityRange", "TotalVolume")],
         aes(PropensityRange, RelVolume, fill=Channel, group=Channel)) +
    geom_col() + geom_smooth(se=F) +
    theme(axis.text.x=element_text(angle=45, hjust=1, size=7)) +
    facet_wrap(. ~ Channel, strip.position = "right") + #, scales = "free_x") +
    xlab("Propensity") +
    scale_y_continuous(name="Volume", labels = scales::percent) +
    scale_fill_Channel + 
    ggtitle("Volume vs. Propensity") 
} else {
  cat("Propensity analysis will only be available when predictor binning data is available.", fill=T)
}
```

# Analysis of Predictors

This analysis focuses on finding which are top predictors that are driving the models. 

The predictors are categorized (by color) by the "source". By default this takes
just the first part before the dot, so this typically distinguishes between
Customer, Account and IH, for example.

## Predictor Importance across all models

Box plots of the predictor importance. Note that the plots use actual
feature importance - so the importance of the predictors in the models. This
is not exactly the same as the univariate predictor performance that you see
in Pega, but typically the two are highly correlated.

```{r Univariate Predictors Boxplot}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  for (c in levels(datamart$modeldata$ConfigurationName)) {
    # applicableModels <- filterLatestSnapshotOnly(datamart$modeldata) [ConfigurationName==c]
    # applicablePredictorBins <- modelPredictorBins[ModelID %in% applicableModels$ModelID & EntryType=="Active"]
    
    if ("PredictorCategory" %in% names(datamart$predictordata)) {
      plt <- plotPredictorImportance(datamart,
                                     limit = params$max_predictors_in_plot, 
                                     filter = function(mdls) {
                                       filterLatestSnapshotOnly(mdls) [ConfigurationName==c]
                                     })
    } else {
      plt <- plotPredictorImportance(datamart,
                                     limit = params$max_predictors_in_plot,
                                     filter = function(mdls) {
                                       filterLatestSnapshotOnly(mdls) [ConfigurationName==c]
                                     })
    }
    if (is.null(plt)) {
      warning("Empty plot for configuration", c)
    } else {
      plt <- plt + ggtitle("Predictor Performance", 
                           subtitle = paste0("(Top", params$max_predictors_in_plot, " ", c, ")")) +
        scale_fill_PredictorCategory
      print(plt)
    }
  }
} else {
  cat("Predictor analysis will only be available when predictor binning data is available.", fill=T)
}
```


## Importance by Predictor Category

Aggregating up to the category of the predictors. This gives a view at
a glance of how well e.g. interaction history or contextual data is doing.

```{r Univariate Predictor Category Boxplot}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  plt <- plotPredictorImportance(datamart,  
                                 categoryAggregateView = T,
                                 facets = c("Channel", "ConfigurationName"),
                                 filter = function(mdls) {filterLatestSnapshotOnly(mdls)} ) +
    scale_fill_PredictorCategory
  print(plt)
} else {
  cat("Predictor analysis will only be available when predictor binning data is available.", fill=T)
}
```

## Bad predictors across all models

See if there are predictors that are just always very bad.

This will help do an analysis to figure out which predictors could be removed. But
note we advise to be careful with predictor removal. Only remove if there is 
clearly no future value to other propositions as well or if there is always a 
related predictor that performs better.

TODO: maybe view this one as performance, not importance

```{r}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  plotPredictorImportance(datamart, facets=NULL, 
                          limit = -params$max_predictors_in_plot,
                          filter = function(mdls) {filterLatestSnapshotOnly(mdls)}) +
    scale_fill_PredictorCategory
} else {
  cat("Predictor analysis will only be available when predictor binning data is available.", fill=T)
}
```

## Number of Active and Inactive Predictors

Showing the number of active and inactive predictors per model. This may not
be that meaningful for a specific customer but it may help to give a ball-park
figure on how many predictors are used by the models.

```{r fig.height=5, fig.width=5}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  typeDistributionPlotData <- 
    merge(unique(filterLatestSnapshotOnly(datamart$predictordata)[EntryType!="Classifier", 
                                                                  c("ModelID", "PredictorName", "EntryType", "Type")]) [, .N, by=c("EntryType", "Type", "ModelID")], 
          unique(filterLatestSnapshotOnly(datamart$modeldata)[, c("ModelID", "ConfigurationName")]), by="ModelID")
  
  plt <-
    ggplot(typeDistributionPlotData, aes(N, Type, color=Type, linetype=EntryType)) + 
    geom_boxplot() +
    scale_color_discrete_diverging(name="Predictor Type") +
    scale_linetype_discrete(name="Status") +
    ylab("")+ xlab("") + 
    facet_wrap(. ~ ConfigurationName, strip.position = "right") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    ggtitle("Number of Predictors per Model", subtitle = "by Type and per model configuration")
  
  plt
} else {
  cat("Predictor analysis will only be available when predictor binning data is available.", fill=T)
}
```


## Predictor Performance across Propositions

A view of predictor performance across all propositions, ordered so that the best performing predictors are at the top and the 
best performing propositions are on the left. Green indicates good performance, red means more problematic - either too low or
too good to be true.


```{r Predictors vs Propos, fig.height=8, fig.width=8}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  for (c in levels(datamart$modeldata$ConfigurationName)) {
    # applicableModels <- filterLatestSnapshotOnly(mdls) [ConfigurationName==c]
    # applicablePredictorBins <- modelPredictorBins[EntryType=="Active" & ModelID %in% applicableModels$ModelID]
    plt <- plotPredictorImportanceHeatmap(datamart, 
                                          limit=params$max_propositions_in_plot,
                                          filter = function(mdls) {
                                            filterLatestSnapshotOnly(mdls) [ConfigurationName==c]
                                          } ) +
      theme(axis.text.y = element_text(size=8),
            axis.text.x = element_text(size=8, angle = 45, hjust = 1),
            strip.text = element_text(size=8))
    if (is.null(plt)) {
      warning("Empty plot for configuration", c)
    } else {
       print(plt)
    }
  }
} else {
  cat("Predictor analysis will only be available when predictor binning data is available.", fill=T)
}
```

# Technical Issues

Many technical issues manifest themselves in one or more of the analyses 
before. A few specific issues are analysed below.

## Missing values (TODO)

TODO: add analysis of predictors with high number of missing values across the models

If a predictor is low performing: are there too many missing values? This could point to a technical problem

## Recent Response counts (TODO)

TODO: analysis of the response counts of the last X days so to show whether
they are increasing from day to day or at least not decreasing.

# Appendix - all the models

```{r}
datamart$modeldata[, list(Snapshots = .N, Responses = max(ResponseCount)), by=c(availableStandardContextKeys, "ModelID")] #[order(ConfigurationName, Name)]
```

A list of all the models is written to a file `r params$modellist` so a script
can iterate over all models and generate off-line model reports for each of 
them.

Generally you will want to apply some filtering, or do this for specific models
only. This can be accomplished in either this script here, or by editing the
generated file.

```{r}
if (params$modellist != "") {
  inclKeys <- availableStandardContextKeys[sapply(availableStandardContextKeys, function(x) {return(length(unique(datamart$modeldata[[x]]))>1)})]
  modelIDandSanitizedNames <- unique(datamart$modeldata[, list(make.names(apply(.SD, 1, function(x){return(paste(x,collapse="_"))}))), 
                                                        by=ModelID, .SDcols=inclKeys])
  
  write.table(modelIDandSanitizedNames, 
              params$modellist, row.names = F, col.names = F, quote=F, sep=";")
}
```

