{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDS (Historical Data Set) Analysis\n",
    "\n",
    "This notebook demonstrates how to work with a sample HDS dataset, explore its structure, and build a simple XGBoost model to show feature importance.\n",
    "\n",
    "It then shows how to combine this data with a sample dataset from the Data Lake and perform an analysis of which features could be considered to pull in to Pega to improve the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import plotly.express as px\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from pdstools.utils import cdh_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Read\n",
    "\n",
    "Read the HDS data. Depending on how the data was extracted and stored, you may need different reading methods using\n",
    "zipfile and/or Polars.\n",
    "\n",
    "We're also casting the data to the appropriate types and dropping some features that shouldnt be used for models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = zipfile.ZipFile(\"../../data/hds.zip\", \"r\")\n",
    "hds_data = (\n",
    "    pl.concat([pl.read_ndjson(archive.open(f)) for f in archive.namelist()])\n",
    "    .rename({\"Customer_C_CIFNBR\": \"Customer_ID\"})\n",
    "    .with_columns(\n",
    "        cdh_utils.parse_pega_date_time_formats(\"Decision_DecisionTime\"),\n",
    "        cdh_utils.parse_pega_date_time_formats(\"Decision_OutcomeTime\"),\n",
    "        cs.ends_with(\"_DaysSince\", \"_pyHistoricalOutcomeCount\").cast(pl.Float64),\n",
    "        pl.col(\n",
    "            [\n",
    "                \"Customer_NetWealth\",\n",
    "                \"Customer_CreditScore\",\n",
    "                \"Customer_CLV_VALUE\",\n",
    "                \"Customer_RelationshipStartDate\",\n",
    "                \"Customer_Date_of_Birth\",\n",
    "                \"Customer_NoOfDependents\"\n",
    "            ]\n",
    "        ).cast(pl.Float64),\n",
    "        cs.starts_with(\"Param_ExtGroup\").cast(pl.Float64),\n",
    "    )\n",
    "    .drop([\"Customer_Gender\", \"Customer_Prefix\"])\n",
    ")\n",
    "hds_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Fields in the HDS dataset\n",
    "\n",
    "The HDS data contains all the payload sent to the ADM models (over a period of time) plus the outcomes (Accepted/Declined/Clicked etc). There are a few categories of fields, that can be identified by their prefix:\n",
    "\n",
    "* \"Customer\" fields, representing the fields/predictors configured in ADM\n",
    "* \"Context\" fields, these are Channel/Direction/Issue/Group/Name, the usual \"context identifiers\" of the ADM models\n",
    "* \"IH\" fields, these are Pega-generated fields derived from Interaction History\n",
    "* Optional \"Param\" fields, also user defined fields/predictors, but configured in the strategies, rather than defined in the ADM model configuration\n",
    "\n",
    "Meta information about the decisions is in Decision and internal fields, containing info about the time of decision, the sample size etc. These are not used in the models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_data_dictionary = (\n",
    "    pl.DataFrame(\n",
    "           {\"Field\" : hds_data.schema.names(),\n",
    "           \"Numeric\" : [x.is_numeric() for x in hds_data.schema.dtypes()],}\n",
    "    )\n",
    "    .with_columns(\n",
    "        Category=pl.when(pl.col(\"Field\").str.contains(\"_\", literal=True))\n",
    "        .then(pl.col(\"Field\").str.replace(r\"([^_]+)_.*\", \"${1}\"))\n",
    "        .otherwise(pl.lit(\"Internal\"))\n",
    "    )\n",
    "    .sort(\"Category\")\n",
    ")\n",
    "hds_data_dictionary.to_pandas().style.hide()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = (\n",
    "    hds_data_dictionary\n",
    "    .group_by(\"Category\", \"Numeric\")\n",
    "    .agg(Count=pl.len())\n",
    "    .sort(\"Category\")\n",
    ")\n",
    "fig = px.bar(\n",
    "    category_counts, #.to_dict(as_series=False),\n",
    "    y=\"Category\",\n",
    "    x=\"Count\",\n",
    "    title=\"Number of Fields by Category\",\n",
    "    color=\"Numeric\",\n",
    "    text=\"Count\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Field Category\",\n",
    "    xaxis_title=\"Number of Fields\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create XGBoost model \n",
    "\n",
    "From this HDS data we create a simple XGBoost model. We create a simple model over all channels and all actions, not split up like ADM does. This could be changed, but the goal here is to get a list of the features ranked by importance, not to create a model that is better than ADM.\n",
    "\n",
    "First, there is data prep to do one-hot encoding and target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data, data_dictionary):\n",
    "    categorical_fields = (\n",
    "        data_dictionary.filter(~pl.col(\"Numeric\"))\n",
    "        .filter((pl.col(\"Category\") != \"Internal\") & (pl.col(\"Category\") != \"Decision\"))\n",
    "        .select(\"Field\")\n",
    "        .to_series()\n",
    "        .to_list()\n",
    "    )\n",
    "    numerical_fields = (\n",
    "        data_dictionary.filter(pl.col(\"Numeric\"))\n",
    "        .filter((pl.col(\"Category\") != \"Internal\") & (pl.col(\"Category\") != \"Decision\"))\n",
    "        .select(\"Field\")\n",
    "        .to_series()\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    print(f\"Categorical fields: {categorical_fields}\")\n",
    "    print(f\"Numerical fields: {numerical_fields}\")\n",
    "\n",
    "    # Convert to pandas for easier encoding with sklearn\n",
    "    data_pd = data.to_pandas()\n",
    "\n",
    "    # Create a copy for modeling\n",
    "    model_data = data_pd.copy()\n",
    "\n",
    "    # Simple encoding for categorical features\n",
    "    for col in categorical_fields:\n",
    "        if col != \"Decision_Outcome\":\n",
    "            # Handle missing values\n",
    "            model_data[col] = model_data[col].fillna(\"missing\")\n",
    "            # Create a simple label encoder\n",
    "            le = LabelEncoder()\n",
    "            model_data[col + \"_encoded\"] = le.fit_transform(model_data[col])\n",
    "\n",
    "    # Encode target variable\n",
    "    le_target = LabelEncoder()\n",
    "    model_data[\"target\"] = le_target.fit_transform(model_data[\"Decision_Outcome\"])\n",
    "\n",
    "    # Show target encoding\n",
    "    target_mapping = dict(zip(le_target.classes_, range(len(le_target.classes_))))\n",
    "    print(f\"\\nTarget encoding: {target_mapping}\")\n",
    "\n",
    "    # Select features and target\n",
    "    feature_cols = [\n",
    "        col for col in model_data.columns if col.endswith(\"_encoded\")\n",
    "    ] + numerical_fields\n",
    "    X = model_data[feature_cols]\n",
    "    y = model_data[\"target\"]\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, le_target, feature_cols\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, target_encoder, feature_cols = data_prep(hds_data, hds_data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(X_train, X_test, y_train, y_test):\n",
    "    # Create and train the model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    print (f\"Model AUC: {round(roc_auc_score(y_test,y_pred), 5)}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_encoder.classes_))\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "classifier = create_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_imp(classifier):\n",
    "    importances = classifier.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importance_df = (\n",
    "        pl.DataFrame({\"Feature\": feature_cols, \"Importance\": importances.tolist()})\n",
    "        .with_columns(\n",
    "            Feature=pl.when(pl.col(\"Feature\").str.ends_with(\"_encoded\"))\n",
    "            .then(pl.col(\"Feature\").str.replace(r\"_encoded$\", \"\"))\n",
    "            .otherwise(pl.col(\"Feature\"))\n",
    "        )\n",
    "        .with_columns(\n",
    "            Category=pl.when(pl.col(\"Feature\").str.contains(\"_\", literal=True))\n",
    "            .then(pl.col(\"Feature\").str.replace(r\"([^_]+)_.*\", \"${1}\"))\n",
    "            .otherwise(pl.lit(\"Internal\"))\n",
    "        )\n",
    "        .sort(\"Importance\", descending=True)\n",
    "    )\n",
    "\n",
    "    # Get top 20 features by importance\n",
    "    top_features_df = feature_importance_df.head(20)\n",
    "\n",
    "    # Get the ordered list of feature names\n",
    "    feature_order = top_features_df[\"Feature\"].to_list()\n",
    "\n",
    "    # Plot feature importances\n",
    "    fig = px.bar(\n",
    "        top_features_df,\n",
    "        x=\"Importance\",\n",
    "        y=\"Feature\",\n",
    "        orientation=\"h\",\n",
    "        title=\"Feature Importance\",\n",
    "        color=\"Category\",\n",
    "        color_discrete_map={\n",
    "            \"Context\": \"orange\",\n",
    "            \"IH\": \"green\",\n",
    "            \"Customer\": \"blue\",\n",
    "            \"Param\": \"lightblue\",\n",
    "            \"DataLake\": \"red\",\n",
    "            \"Internal\": \"gray\",\n",
    "            \"Decision\": \"purple\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Importance\",\n",
    "        yaxis_title=\"Feature\",\n",
    "        yaxis=dict(\n",
    "            categoryorder=\"array\",\n",
    "            categoryarray=feature_order,\n",
    "            autorange=\"reversed\",\n",
    "            dtick=1,\n",
    "        ),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "show_feature_imp(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding new Features from the Data Lake\n",
    "\n",
    "Now, suppose you have external data from your data lake that you want to consider adding to Pega to improve the performance of your models.\n",
    "\n",
    "If you have such data, you can merge it with the HDS data and run the model again to see how these features fare against what ADM already uses.\n",
    "\n",
    "Such data is typically time-stamped, so we need to be careful to only pull in data from before the decisions were made. \n",
    "\n",
    "## Create (fake) External Data\n",
    "\n",
    "We first create an example of external data. All features are captured over time there, so there is a feature name, a timestamp, and a value.\n",
    "\n",
    "This code (and resulting data) are just an example. You can use any data you want, we just highlight the structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(101)\n",
    "datalake_fake_data = hds_data.with_columns(\n",
    "    DataLake_BadFeature=pl.Series([random.random() for _ in range(hds_data.height)]),\n",
    "    DataLake_GoodFeature=(pl.col(\"Decision_Outcome\") == \"Accepted\") * 0.9\n",
    "    + pl.Series([random.random() for _ in range(hds_data.height)]) * 0.1,\n",
    ").select(\n",
    "    [\n",
    "        pl.col(\"Customer_ID\"),\n",
    "        pl.col(\"Decision_DecisionTime\").dt.truncate(\"1d\").alias(\"SnapshotTime\"),\n",
    "        pl.col(\"DataLake_BadFeature\"),\n",
    "        pl.col(\"DataLake_GoodFeature\"),\n",
    "    ]\n",
    ").group_by(\n",
    "    [\"Customer_ID\", \"SnapshotTime\"]\n",
    ").agg(\n",
    "    cs.all().mean()\n",
    ").sort([\"Customer_ID\", \"SnapshotTime\"])\n",
    "\n",
    "datalake_fake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining that data with the HDS data is straightforward: we match by customer ID and timestamp, but need to be careful to avoid leakage, so we only join in data for a particular customer from the data lake that is the latest snapshot before the timestamp of the HDS dataset. \n",
    "\n",
    "Polars provides a convenient way to do this with the join_asof function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = hds_data.join_asof(\n",
    "    datalake_fake_data,\n",
    "    left_on=\"Decision_DecisionTime\",\n",
    "    right_on=\"SnapshotTime\",\n",
    "    by=\"Customer_ID\",\n",
    ")\n",
    "augmented_data_dictionary = pl.concat(\n",
    "    [\n",
    "        hds_data_dictionary,\n",
    "        pl.DataFrame(\n",
    "            {\n",
    "                \"Field\": [\"DataLake_BadFeature\", \"DataLake_GoodFeature\"],\n",
    "                \"Numeric\": [True, True],\n",
    "                \"Category\": [\"DataLake\", \"DataLake\"],\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, target_encoder, feature_cols = data_prep(augmented_data, augmented_data_dictionary)\n",
    "classifier = create_classifier(X_train, X_test, y_train, y_test)\n",
    "show_feature_imp(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The resulting feature importance shows how the new \"GoodFeature\" is on top of the list of features, and the \"BadFeature\" is at the bottom. \n",
    "\n",
    "You would consider adding the \"GoodFeature\" to Pega, and not the \"BadFeature\".\n",
    "\n",
    "As an improvement (TODO) we should also consider feature correlation, so if there are multiple \"good\" features, bring in only the ones that are not strongly correlated.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
