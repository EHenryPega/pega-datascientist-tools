{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDS (Historical Data Set) Analysis with XGBoost\n",
    "\n",
    "This notebook demonstrates how to work with a sample HDS dataset, explore its structure, and build a simple XGBoost model to predict outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import great_tables as gt\n",
    "from pdstools.utils import cdh_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Read\n",
    "\n",
    "Read the HDS data. Depending on how the data was extracted and stored, you may need different reading methods using\n",
    "zipfile and/or Polars.\n",
    "\n",
    "We're also casting the data to the appropriate types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = zipfile.ZipFile(\"../../data/hds.zip\", \"r\")\n",
    "hds_data = pl.concat(\n",
    "    [pl.read_ndjson(archive.open(f)) for f in archive.namelist()]\n",
    ").with_columns(\n",
    "    cdh_utils.parse_pega_date_time_formats(\"Decision_DecisionTime\"),\n",
    "    cdh_utils.parse_pega_date_time_formats(\"Decision_OutcomeTime\"),\n",
    "    cs.ends_with(\"_DaysSince\", \"_pyHistoricalOutcomeCount\").cast(pl.Float64),\n",
    "    pl.col(\n",
    "        [\n",
    "            \"Customer_NetWealth\",\n",
    "            \"Customer_CreditScore\",\n",
    "            \"Customer_CLV_VALUE\",\n",
    "            \"Customer_RelationshipStartDate\",\n",
    "            \"Customer_Date_of_Birth\",\n",
    "            # \"Customer_TotalLiabilities\",\n",
    "        ]\n",
    "    ).cast(pl.Float64),\n",
    "    cs.starts_with(\"Param_ExtGroup\").cast(pl.Float64),\n",
    "    pl.col([\"Customer_NoOfDependents\"]).cast(pl.Float64),\n",
    ")\n",
    "hds_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available fields\n",
    "\n",
    "See Pega doc for an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = (\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"Field\": hds_data.columns,\n",
    "        }\n",
    "    )\n",
    "    .with_columns(\n",
    "        Category=pl.when(pl.col(\"Field\").str.contains(\"_\", literal=True))\n",
    "        .then(pl.col(\"Field\").str.replace(r\"([^_]+)_.*\", \"${1}\"))\n",
    "        .otherwise(pl.lit(\"Internal\"))\n",
    "    )\n",
    "    .group_by(\"Category\")\n",
    "    .agg(Count=pl.len())\n",
    "    .sort(\"Category\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    category_counts, #.to_dict(as_series=False),\n",
    "    y=\"Category\",\n",
    "    x=\"Count\",\n",
    "    title=\"Number of Fields by Category\",\n",
    "    color=\"Category\",\n",
    "    text=\"Count\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Field Category\",\n",
    "    xaxis_title=\"Number of Fields\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create XGBoost model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling - simplified approach without map_dict\n",
    "if 'Decision_Outcome' in hds_data.columns:\n",
    "    # Identify categorical and numerical fields\n",
    "    categorical_fields = []\n",
    "    numerical_fields = []\n",
    "    \n",
    "    for column in hds_data.columns:\n",
    "        if hds_data[column].dtype in [pl.Int64, pl.Float64]:\n",
    "            numerical_fields.append(column)\n",
    "        else:\n",
    "            categorical_fields.append(column)\n",
    "    \n",
    "    print(f\"Categorical fields: {categorical_fields}\")\n",
    "    print(f\"Numerical fields: {numerical_fields}\")\n",
    "    \n",
    "    # Convert to pandas for easier encoding with sklearn\n",
    "    hds_data_pd = hds_data.to_pandas()\n",
    "    \n",
    "    # Create a copy for modeling\n",
    "    model_data = hds_data_pd.copy()\n",
    "    \n",
    "    # Simple encoding for categorical features\n",
    "    for col in categorical_fields:\n",
    "        if col != 'Decision_Outcome':\n",
    "            # Handle missing values\n",
    "            model_data[col] = model_data[col].fillna('missing')\n",
    "            # Create a simple label encoder\n",
    "            le = LabelEncoder()\n",
    "            model_data[col + '_encoded'] = le.fit_transform(model_data[col])\n",
    "    \n",
    "    # Encode target variable\n",
    "    le_target = LabelEncoder()\n",
    "    model_data['target'] = le_target.fit_transform(model_data['Decision_Outcome'])\n",
    "    \n",
    "    # Show target encoding\n",
    "    target_mapping = dict(zip(le_target.classes_, range(len(le_target.classes_))))\n",
    "    print(f\"\\nTarget encoding: {target_mapping}\")\n",
    "    \n",
    "    # Select features and target\n",
    "    feature_cols = [col for col in model_data.columns if col.endswith('_encoded')] + numerical_fields\n",
    "    X = model_data[feature_cols]\n",
    "    y = model_data['target']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train XGBoost model\n",
    "if 'Decision_Outcome' in hds_data.columns:\n",
    "    # Create and train the model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and evaluate\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le_target.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "if \"Decision_Outcome\" in hds_data.columns:\n",
    "    # Get feature importances\n",
    "    importances = xgb_model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importance_df = (\n",
    "        pl.DataFrame({\"Feature\": feature_cols, \"Importance\": importances.tolist()})\n",
    "        .with_columns(\n",
    "            Feature = pl.when(pl.col(\"Feature\").str.ends_with(\"_encoded\")).then(pl.col(\"Feature\").str.replace(r\"_encoded$\", \"\")).otherwise(pl.col(\"Feature\"))\n",
    "        )\n",
    "        .with_columns(\n",
    "            Category=pl.when(pl.col(\"Feature\").str.contains(\"_\", literal=True))\n",
    "            .then(pl.col(\"Feature\").str.replace(r\"([^_]+)_.*\", \"${1}\"))\n",
    "            .otherwise(pl.lit(\"Internal\"))\n",
    "        )\n",
    "        .sort(\"Importance\", descending=True)\n",
    "    )\n",
    "\n",
    "    # Plot feature importances\n",
    "    fig = px.bar(\n",
    "        feature_importance_df.head(20),  # .to_dict(as_series=False),\n",
    "        x=\"Importance\",\n",
    "        y=\"Feature\",\n",
    "        orientation=\"h\",\n",
    "        title=\"Feature Importance\",\n",
    "        color=\"Category\",\n",
    "        # color_continuous_scale=\"Viridis\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Importance\",\n",
    "        yaxis_title=\"Feature\",\n",
    "        yaxis=dict(autorange=\"reversed\", dtick=1, type='category'),\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding new Features\n",
    "\n",
    "Now, suppose you have some external data from your data lake that you want to consider adding to Pega to improve the performance of your models.\n",
    "\n",
    "If you have such data, you can merge it with the HDS data and run the model again to see how these features fare against what ADM already uses.\n",
    "\n",
    "Such data is typically time-stamped, and we need to be careful to only pull in data from before the decisions were made. \n",
    "\n",
    "## External data example\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
