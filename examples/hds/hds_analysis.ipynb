{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDS (Historical Data Set) Analysis with XGBoost\n",
    "\n",
    "This notebook demonstrates how to work with a sample HDS dataset, explore its structure, and build a simple XGBoost model to predict outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import great_tables as gt\n",
    "from pdstools.utils import cdh_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Read\n",
    "\n",
    "Read the HDS data. Depending on how the data was extracted and stored, you may need different reading methods using\n",
    "zipfile and/or Polars.\n",
    "\n",
    "We're also casting the data to the appropriate types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = zipfile.ZipFile(\"../../data/hds.zip\", \"r\")\n",
    "hds_data = pl.concat(\n",
    "    [pl.read_ndjson(archive.open(f)) for f in archive.namelist()]\n",
    ").rename({'Customer_C_CIFNBR' : 'Customer_ID'}).with_columns(\n",
    "    cdh_utils.parse_pega_date_time_formats(\"Decision_DecisionTime\"),\n",
    "    cdh_utils.parse_pega_date_time_formats(\"Decision_OutcomeTime\"),\n",
    "    cs.ends_with(\"_DaysSince\", \"_pyHistoricalOutcomeCount\").cast(pl.Float64),\n",
    "    pl.col(\n",
    "        [\n",
    "            \"Customer_NetWealth\",\n",
    "            \"Customer_CreditScore\",\n",
    "            \"Customer_CLV_VALUE\",\n",
    "            \"Customer_RelationshipStartDate\",\n",
    "            \"Customer_Date_of_Birth\",\n",
    "            # \"Customer_TotalLiabilities\",\n",
    "        ]\n",
    "    ).cast(pl.Float64),\n",
    "    cs.starts_with(\"Param_ExtGroup\").cast(pl.Float64),\n",
    "    pl.col([\"Customer_NoOfDependents\"]).cast(pl.Float64),\n",
    ")\n",
    "hds_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available fields\n",
    "\n",
    "See Pega doc for an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hds_data_dictionary = (\n",
    "    pl.DataFrame(\n",
    "           {\"Field\" : hds_data.schema.names(),\n",
    "           \"Numeric\" : [x.is_numeric() for x in hds_data.schema.dtypes()],}\n",
    "    )\n",
    "    .with_columns(\n",
    "        Category=pl.when(pl.col(\"Field\").str.contains(\"_\", literal=True))\n",
    "        .then(pl.col(\"Field\").str.replace(r\"([^_]+)_.*\", \"${1}\"))\n",
    "        .otherwise(pl.lit(\"Internal\"))\n",
    "    )\n",
    "    .sort(\"Category\")\n",
    ")\n",
    "hds_data_dictionary.to_pandas().style.hide()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = (\n",
    "    hds_data_dictionary\n",
    "    .group_by(\"Category\", \"Numeric\")\n",
    "    .agg(Count=pl.len())\n",
    "    .sort(\"Category\")\n",
    ")\n",
    "fig = px.bar(\n",
    "    category_counts, #.to_dict(as_series=False),\n",
    "    y=\"Category\",\n",
    "    x=\"Count\",\n",
    "    title=\"Number of Fields by Category\",\n",
    "    color=\"Numeric\",\n",
    "    text=\"Count\",\n",
    "    orientation=\"h\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Field Category\",\n",
    "    xaxis_title=\"Number of Fields\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create XGBoost model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data, data_dictionary):\n",
    "    categorical_fields = (\n",
    "        data_dictionary.filter(~pl.col(\"Numeric\"))\n",
    "        .filter((pl.col(\"Category\") != \"Internal\") & (pl.col(\"Category\") != \"Decision\"))\n",
    "        .select(\"Field\")\n",
    "        .to_series()\n",
    "        .to_list()\n",
    "    )\n",
    "    numerical_fields = (\n",
    "        data_dictionary.filter(pl.col(\"Numeric\"))\n",
    "        .filter((pl.col(\"Category\") != \"Internal\") & (pl.col(\"Category\") != \"Decision\"))\n",
    "        .select(\"Field\")\n",
    "        .to_series()\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    print(f\"Categorical fields: {categorical_fields}\")\n",
    "    print(f\"Numerical fields: {numerical_fields}\")\n",
    "\n",
    "    # Convert to pandas for easier encoding with sklearn\n",
    "    data_pd = data.to_pandas()\n",
    "\n",
    "    # Create a copy for modeling\n",
    "    model_data = data_pd.copy()\n",
    "\n",
    "    # Simple encoding for categorical features\n",
    "    for col in categorical_fields:\n",
    "        if col != \"Decision_Outcome\":\n",
    "            # Handle missing values\n",
    "            model_data[col] = model_data[col].fillna(\"missing\")\n",
    "            # Create a simple label encoder\n",
    "            le = LabelEncoder()\n",
    "            model_data[col + \"_encoded\"] = le.fit_transform(model_data[col])\n",
    "\n",
    "    # Encode target variable\n",
    "    le_target = LabelEncoder()\n",
    "    model_data[\"target\"] = le_target.fit_transform(model_data[\"Decision_Outcome\"])\n",
    "\n",
    "    # Show target encoding\n",
    "    target_mapping = dict(zip(le_target.classes_, range(len(le_target.classes_))))\n",
    "    print(f\"\\nTarget encoding: {target_mapping}\")\n",
    "\n",
    "    # Select features and target\n",
    "    feature_cols = [\n",
    "        col for col in model_data.columns if col.endswith(\"_encoded\")\n",
    "    ] + numerical_fields\n",
    "    X = model_data[feature_cols]\n",
    "    y = model_data[\"target\"]\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, le_target, feature_cols\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, target_encoder, feature_cols = data_prep(hds_data, hds_data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(X_train, X_test, y_train, y_test):\n",
    "    # Create and train the model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    print (f\"Model AUC: {round(roc_auc_score(y_test,y_pred), 5)}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_encoder.classes_))\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "classifier = create_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_imp(classifier):\n",
    "    importances = classifier.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importance_df = (\n",
    "        pl.DataFrame({\"Feature\": feature_cols, \"Importance\": importances.tolist()})\n",
    "        .with_columns(\n",
    "            Feature = pl.when(pl.col(\"Feature\").str.ends_with(\"_encoded\")).then(pl.col(\"Feature\").str.replace(r\"_encoded$\", \"\")).otherwise(pl.col(\"Feature\"))\n",
    "        )\n",
    "        .with_columns(\n",
    "            Category=pl.when(pl.col(\"Feature\").str.contains(\"_\", literal=True))\n",
    "            .then(pl.col(\"Feature\").str.replace(r\"([^_]+)_.*\", \"${1}\"))\n",
    "            .otherwise(pl.lit(\"Internal\"))\n",
    "        )\n",
    "        .sort(\"Importance\", descending=True)\n",
    "    )\n",
    "\n",
    "    # Plot feature importances\n",
    "    fig = px.bar(\n",
    "        feature_importance_df.head(20),  # .to_dict(as_series=False),\n",
    "        x=\"Importance\",\n",
    "        y=\"Feature\",\n",
    "        orientation=\"h\",\n",
    "        title=\"Feature Importance\",\n",
    "        color=\"Category\",\n",
    "        # color_continuous_scale=\"Viridis\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Importance\",\n",
    "        yaxis_title=\"Feature\",\n",
    "        yaxis=dict(autorange=\"reversed\", dtick=1, type='category'),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "show_feature_imp(classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding new Features\n",
    "\n",
    "Now, suppose you have some external data from your data lake that you want to consider adding to Pega to improve the performance of your models.\n",
    "\n",
    "If you have such data, you can merge it with the HDS data and run the model again to see how these features fare against what ADM already uses.\n",
    "\n",
    "Such data is typically time-stamped, and we need to be careful to only pull in data from before the decisions were made. \n",
    "\n",
    "## External data example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're simulating some features from an external datalake here. One is a \"bad\" feature that has just random values, the other is artificially created to be a very good feature. \n",
    "\n",
    "We re-do the data prep and model building and expect both to show at the top and bottom of the feature importance list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "datalake_fake_data = hds_data.with_columns(\n",
    "    DataLake_BadFeature=pl.Series([random.random() for _ in range(hds_data.height)]),\n",
    "    DataLake_GoodFeature=(pl.col(\"Decision_Outcome\") == \"Accepted\") * 0.9\n",
    "    + pl.Series([random.random() for _ in range(hds_data.height)]) * 0.1,\n",
    ").select(\n",
    "    [\n",
    "        pl.col(\"Customer_ID\"),\n",
    "        pl.col(\"Decision_DecisionTime\").dt.truncate(\"1d\"),\n",
    "        pl.col(\"DataLake_BadFeature\"),\n",
    "        pl.col(\"DataLake_GoodFeature\"),\n",
    "    ]\n",
    ").group_by(\n",
    "    [\"Customer_ID\", \"Decision_DecisionTime\"]\n",
    ").agg(\n",
    "    cs.all().mean()\n",
    ").sort([\"Customer_ID\", \"Decision_DecisionTime\"])\n",
    "\n",
    "datalake_fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = hds_data.join_asof(datalake_fake_data, on=\"Decision_DecisionTime\", by=\"Customer_ID\")\n",
    "augmented_data_dictionary = pl.concat([\n",
    "    hds_data_dictionary,\n",
    "    pl.DataFrame({\"Field\" : [\"DataLake_BadFeature\", \"DataLake_GoodFeature\"],\t\n",
    "                  \"Numeric\" : [True, True],\t\n",
    "                  \"Category\" : [\"DataLake\", \"DataLake\"]})]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, target_encoder, feature_cols = data_prep(augmented_data, augmented_data_dictionary)\n",
    "classifier = create_classifier(X_train, X_test, y_train, y_test)\n",
    "show_feature_imp(classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
